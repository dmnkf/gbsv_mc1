{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Image and Signal Processing: A Journey Through Korea](#toc1_)    \n",
    "- 2. [Metrics and Tools](#toc2_)    \n",
    "  - 2.1. [PSNR (Peak Signal-to-Noise Ratio)](#toc2_1_)    \n",
    "    - 2.1.1. [Formula](#toc2_1_1_)    \n",
    "  - 2.2. [SSIM (Structural Similarity Index)](#toc2_2_)    \n",
    "    - 2.2.1. [Formula](#toc2_2_1_)    \n",
    "  - 2.3. [Histogram Analysis](#toc2_3_)    \n",
    "    - 2.3.1. [Interpretation](#toc2_3_1_)    \n",
    "  - 2.4. [Histogram Equalization](#toc2_4_)    \n",
    "    - 2.4.1. [Formula](#toc2_4_1_)    \n",
    "  - 2.5. [Median Filter](#toc2_5_)    \n",
    "    - 2.5.1. [Method](#toc2_5_1_)    \n",
    "  - 2.6. [Bilateral Filter](#toc2_6_)    \n",
    "    - 2.6.1. [Method](#toc2_6_1_)    \n",
    "  - 2.7. [CLAHE (Contrast Limited Adaptive Histogram Equalization)](#toc2_7_)    \n",
    "    - 2.7.1. [Formula](#toc2_7_1_)    \n",
    "  - 2.8. [Lab Color Space Equalization](#toc2_8_)    \n",
    "    - 2.8.1. [Formula](#toc2_8_1_)    \n",
    "  - 2.9. [Unsharp Mask](#toc2_9_)    \n",
    "    - 2.9.1. [Formula](#toc2_9_1_)    \n",
    "  - 2.10. [Laplacian of Gaussian (LoG) Filter](#toc2_10_)    \n",
    "    - 2.10.1. [Formula](#toc2_10_1_)    \n",
    "    - 2.10.2. [Kernel](#toc2_10_2_)    \n",
    "    - 2.10.3. [Applications](#toc2_10_3_)    \n",
    "  - 2.11. [Signal-to-Noise Ratio (SNR)](#toc2_11_)    \n",
    "    - 2.11.1. [Formula](#toc2_11_1_)    \n",
    "    - 2.11.2. [Applications](#toc2_11_2_)    \n",
    "  - 2.12. [Spectral Contrast](#toc2_12_)    \n",
    "    - 2.12.1. [Formula](#toc2_12_1_)    \n",
    "    - 2.12.2. [Applications](#toc2_12_2_)    \n",
    "  - 2.13. [Harmonic-to-Noise Ratio (HNR)](#toc2_13_)    \n",
    "    - 2.13.1. [Formula](#toc2_13_1_)    \n",
    "    - 2.13.2. [Applications](#toc2_13_2_)    \n",
    "  - 2.14. [REPET-SIM Method](#toc2_14_)    \n",
    "    - 2.14.1. [Overview](#toc2_14_1_)    \n",
    "    - 2.14.2. [Short-Time Fourier Transform (STFT)](#toc2_14_2_)    \n",
    "    - 2.14.3. [Non-local Filtering and Soft Mask](#toc2_14_3_)    \n",
    "    - 2.14.4. [Soft-masking to Isolate Vocals and Music](#toc2_14_4_)    \n",
    "    - 2.14.5. [Applications](#toc2_14_5_)    \n",
    "  - 2.15. [Laplacian Kernel](#toc2_15_)    \n",
    "    - 2.15.1. [Formula](#toc2_15_1_)    \n",
    "  - 2.16. [Gaussian Kernel](#toc2_16_)    \n",
    "    - 2.16.1. [Formula](#toc2_16_1_)    \n",
    "  - 2.17. [Canny Edge Detection](#toc2_17_)    \n",
    "    - 2.17.1. [Formula](#toc2_17_1_)    \n",
    "  - 2.18. [Probabilistic Hough Line Transform](#toc2_18_)    \n",
    "    - 2.18.1. [Formula](#toc2_18_1_)    \n",
    "- 3. [Helper Functions](#toc3_)    \n",
    "- 4. [Basics Image and Signal Processing (LE1)](#toc4_)    \n",
    "  - 4.1. [Image Properties](#toc4_1_)    \n",
    "    - 4.1.1. [Problem](#toc4_1_1_)    \n",
    "    - 4.1.2. [Experiment 1: Contrast Enhancement Through Various Techniques](#toc4_1_2_)    \n",
    "      - 4.1.2.1. [Objective](#toc4_1_2_1_)    \n",
    "      - 4.1.2.2. [Approach](#toc4_1_2_2_)    \n",
    "      - 4.1.2.3. [Metrics and Tools](#toc4_1_2_3_)    \n",
    "        - 4.1.2.3.1. [PSNR (Peak Signal-to-Noise Ratio)](#toc4_1_2_3_1_)    \n",
    "        - 4.1.2.3.2. [Histogram Analysis](#toc4_1_2_3_2_)    \n",
    "        - 4.1.2.3.3. [Results and Analysis](#toc4_1_2_3_3_)    \n",
    "    - 4.1.3. [Experiment 2: Comparative Noise Reduction on Equalized Image](#toc4_1_3_)    \n",
    "      - 4.1.3.1. [Objective](#toc4_1_3_1_)    \n",
    "      - 4.1.3.2. [Approach](#toc4_1_3_2_)    \n",
    "      - 4.1.3.3. [Metrics and Tools](#toc4_1_3_3_)    \n",
    "        - 4.1.3.3.1. [PSNR (Peak Signal-to-Noise Ratio)](#toc4_1_3_3_1_)    \n",
    "        - 4.1.3.3.2. [SSIM (Structural Similarity Index)](#toc4_1_3_3_2_)    \n",
    "        - 4.1.3.3.3. [Histogram Analysis](#toc4_1_3_3_3_)    \n",
    "      - 4.1.3.4. [Methods](#toc4_1_3_4_)    \n",
    "      - 4.1.3.5. [Results and Analysis](#toc4_1_3_5_)    \n",
    "    - 4.1.4. [Experiment 3: Enhancing Image Sharpness](#toc4_1_4_)    \n",
    "      - 4.1.4.1. [Objective](#toc4_1_4_1_)    \n",
    "      - 4.1.4.2. [Approach](#toc4_1_4_2_)    \n",
    "      - 4.1.4.3. [Metrics and Tools](#toc4_1_4_3_)    \n",
    "        - 4.1.4.3.1. [PSNR (Peak Signal-to-Noise Ratio)](#toc4_1_4_3_1_)    \n",
    "        - 4.1.4.3.2. [SSIM (Structural Similarity Index)](#toc4_1_4_3_2_)    \n",
    "        - 4.1.4.3.3. [Histogram Analysis](#toc4_1_4_3_3_)    \n",
    "      - 4.1.4.4. [Methods](#toc4_1_4_4_)    \n",
    "      - 4.1.4.5. [Results and Analysis](#toc4_1_4_5_)    \n",
    "        - 4.1.4.5.1. [Unsharp Mask](#toc4_1_4_5_1_)    \n",
    "        - 4.1.4.5.2. [Laplacian of Gaussian](#toc4_1_4_5_2_)    \n",
    "        - 4.1.4.5.3. [PSNR and SSIM Metrics](#toc4_1_4_5_3_)    \n",
    "        - 4.1.4.5.4. [Histogram Analysis](#toc4_1_4_5_4_)    \n",
    "        - 4.1.4.5.5. [Performance](#toc4_1_4_5_5_)    \n",
    "    - 4.1.5. [Conclusion](#toc4_1_5_)    \n",
    "  - 4.2. [Signal properties](#toc4_2_)    \n",
    "    - 4.2.1. [Data](#toc4_2_1_)    \n",
    "    - 4.2.2. [Experiment 1: Karaoke Time! Extracting Vocals and Music for Karaoke Versions using REPET-SIM Method](#toc4_2_2_)    \n",
    "      - 4.2.2.1. [Problem](#toc4_2_2_1_)    \n",
    "      - 4.2.2.2. [Objective](#toc4_2_2_2_)    \n",
    "      - 4.2.2.3. [Approach](#toc4_2_2_3_)    \n",
    "      - 4.2.2.4. [Metrics and Tools](#toc4_2_2_4_)    \n",
    "        - 4.2.2.4.1. [Signal-to-Noise Ratio (SNR)](#toc4_2_2_4_1_)    \n",
    "        - 4.2.2.4.2. [Spectral Contrast](#toc4_2_2_4_2_)    \n",
    "        - 4.2.2.4.3. [Audio Playback](#toc4_2_2_4_3_)    \n",
    "      - 4.2.2.5. [Methods](#toc4_2_2_5_)    \n",
    "      - 4.2.2.6. [What Type of X by Jessi](#toc4_2_2_6_)    \n",
    "      - 4.2.2.7. [Dun Dun by Everglow](#toc4_2_2_7_)    \n",
    "      - 4.2.2.8. [Results and Analysis](#toc4_2_2_8_)    \n",
    "        - 4.2.2.8.1. [Metrics Analysis](#toc4_2_2_8_1_)    \n",
    "        - 4.2.2.8.2. [Spectrogram Plots](#toc4_2_2_8_2_)    \n",
    "        - 4.2.2.8.3. [Effectiveness of Vocal and Music Separation](#toc4_2_2_8_3_)    \n",
    "        - 4.2.2.8.4. [Challenges and Limitations](#toc4_2_2_8_4_)    \n",
    "        - 4.2.2.8.5. [Summary and Future Directions](#toc4_2_2_8_5_)    \n",
    "    - 4.2.3. [Experiment 2: Frequency Analysis of Samulnori Music Using Bandpass Filtering](#toc4_2_3_)    \n",
    "      - 4.2.3.1. [Problem](#toc4_2_3_1_)    \n",
    "      - 4.2.3.2. [Objective](#toc4_2_3_2_)    \n",
    "      - 4.2.3.3. [Approach](#toc4_2_3_3_)    \n",
    "      - 4.2.3.4. [Metrics and Tools](#toc4_2_3_4_)    \n",
    "        - 4.2.3.4.1. [Signal-to-Noise Ratio (SNR)](#toc4_2_3_4_1_)    \n",
    "        - 4.2.3.4.2. [Frequency Matching Score](#toc4_2_3_4_2_)    \n",
    "      - 4.2.3.5. [Methods](#toc4_2_3_5_)    \n",
    "      - 4.2.3.6. [Results and Analysis](#toc4_2_3_6_)    \n",
    "    - 4.2.4. [Experiment 3: Waveform Analysis and Transformation of \"Dun Dun\" Remix](#toc4_2_4_)    \n",
    "      - 4.2.4.1. [Problem](#toc4_2_4_1_)    \n",
    "      - 4.2.4.2. [Objective](#toc4_2_4_2_)    \n",
    "      - 4.2.4.3. [Approach](#toc4_2_4_3_)    \n",
    "      - 4.2.4.4. [Metrics and Tools](#toc4_2_4_4_)    \n",
    "        - 4.2.4.4.1. [Waveform Visualization](#toc4_2_4_4_1_)    \n",
    "        - 4.2.4.4.2. [Harmonic-to-Noise Ratio (HNR)](#toc4_2_4_4_2_)    \n",
    "      - 4.2.4.5. [Results and Analysis](#toc4_2_4_5_)    \n",
    "        - 4.2.4.5.1. [Waveform Visualization](#toc4_2_4_5_1_)    \n",
    "        - 4.2.4.5.2. [Spectrogram Analysis](#toc4_2_4_5_2_)    \n",
    "        - 4.2.4.5.3. [Harmonic-to-Noise Ratio (HNR)](#toc4_2_4_5_3_)    \n",
    "        - 4.2.4.5.4. [Methodical Amplitude Modulation](#toc4_2_4_5_4_)    \n",
    "        - 4.2.4.5.5. [Conclusions](#toc4_2_4_5_5_)    \n",
    "  - 4.3. [Experiment 4: Demonstrating the Nyquist Theorem using Samulnori Music](#toc4_3_)    \n",
    "    - 4.3.1. [Problem](#toc4_3_1_)    \n",
    "    - 4.3.2. [Objective](#toc4_3_2_)    \n",
    "    - 4.3.3. [Approach](#toc4_3_3_)    \n",
    "    - 4.3.4. [Metrics and Tools](#toc4_3_4_)    \n",
    "      - 4.3.4.1. [Waveform Similarity Score](#toc4_3_4_1_)    \n",
    "    - 4.3.5. [Methods](#toc4_3_5_)    \n",
    "    - 4.3.6. [Reflection: Nyquist Theorem and Samulnori Music](#toc4_3_6_)    \n",
    "      - 4.3.6.1. [Nyquist Theorem Validation](#toc4_3_6_1_)    \n",
    "      - 4.3.6.2. [Observations from the Plot](#toc4_3_6_2_)    \n",
    "      - 4.3.6.3. [Possible Reasons for Discrepancy](#toc4_3_6_3_)    \n",
    "      - 4.3.6.4. [Conclusion](#toc4_3_6_4_)    \n",
    "  - 4.4. [Normalization and Standardization](#toc4_4_)    \n",
    "    - 4.4.1. [Analysis: Image Normalization and Standardization](#toc4_4_1_)    \n",
    "      - 4.4.1.1. [Normalization](#toc4_4_1_1_)    \n",
    "      - 4.4.1.2. [Standardization](#toc4_4_1_2_)    \n",
    "        - 4.4.1.2.1. [Adjusted Standardization](#toc4_4_1_2_1_)    \n",
    "      - 4.4.1.3. [Difference Maps](#toc4_4_1_3_)    \n",
    "      - 4.4.1.4. [Conclusion](#toc4_4_1_4_)    \n",
    "    - 4.4.2. [Analysis: Signal Normalization and Standardization](#toc4_4_2_)    \n",
    "      - 4.4.2.1. [Normalization](#toc4_4_2_1_)    \n",
    "      - 4.4.2.2. [Standardization](#toc4_4_2_2_)    \n",
    "      - 4.4.2.3. [Why This Happens](#toc4_4_2_3_)    \n",
    "      - 4.4.2.4. [Conclusion](#toc4_4_2_4_)    \n",
    "- 5. [convolution/filtering in image and signal (LE2)](#toc5_)    \n",
    "  - 5.1. [Filtering in the spatial domain](#toc5_1_)    \n",
    "    - 5.1.1. [Choice of Kernals](#toc5_1_1_)    \n",
    "      - 5.1.1.1. [Image Filtering](#toc5_1_1_1_)    \n",
    "      - 5.1.1.2. [Audio Filtering](#toc5_1_1_2_)    \n",
    "    - 5.1.2. [Experiment](#toc5_1_2_)    \n",
    "      - 5.1.2.1. [Metric for Comparing Images and Signals](#toc5_1_2_1_)    \n",
    "      - 5.1.2.2. [Data for Testing](#toc5_1_2_2_)    \n",
    "      - 5.1.2.3. [Analysis: Image Filtering in Spatial Domain](#toc5_1_2_3_)    \n",
    "        - 5.1.2.3.1. [Choice of Kernels](#toc5_1_2_3_1_)    \n",
    "        - 5.1.2.3.2. [Metrics: SSIM and MSE](#toc5_1_2_3_2_)    \n",
    "        - 5.1.2.3.3. [Interpretation of Metrics](#toc5_1_2_3_3_)    \n",
    "        - 5.1.2.3.4. [Scalability for Big Data](#toc5_1_2_3_4_)    \n",
    "        - 5.1.2.3.5. [Conclusion](#toc5_1_2_3_5_)    \n",
    "      - 5.1.2.4. [Audio Filtering Analysis](#toc5_1_2_4_)    \n",
    "        - 5.1.2.4.1. [Laplacian Filtered Audio (Sharpened)](#toc5_1_2_4_1_)    \n",
    "        - 5.1.2.4.2. [Gaussian Filtered Audio (Denoised)](#toc5_1_2_4_2_)    \n",
    "        - 5.1.2.4.3. [Spectrograms](#toc5_1_2_4_3_)    \n",
    "        - 5.1.2.4.4. [Scalability for Big Data](#toc5_1_2_4_4_)    \n",
    "        - 5.1.2.4.5. [Conclusion](#toc5_1_2_4_5_)    \n",
    "  - 5.2. [Filtering in the spectral domain](#toc5_2_)    \n",
    "    - 5.2.1. [Experiment 1: Redundancy Cleanup](#toc5_2_1_)    \n",
    "      - 5.2.1.1. [Methods](#toc5_2_1_1_)    \n",
    "        - 5.2.1.1.1. [Spectral Filtering for 1D Signals](#toc5_2_1_1_1_)    \n",
    "        - 5.2.1.1.2. [Spectral Filtering for 2D Images](#toc5_2_1_1_2_)    \n",
    "      - 5.2.1.2. [Data](#toc5_2_1_2_)    \n",
    "      - 5.2.1.3. [Reflection and Analysis on Spectral Filtering Task](#toc5_2_1_3_)    \n",
    "        - 5.2.1.3.1. [Image Data Results](#toc5_2_1_3_1_)    \n",
    "        - 5.2.1.3.2. [Audio Data Results](#toc5_2_1_3_2_)    \n",
    "        - 5.2.1.3.3. [Overall Analysis](#toc5_2_1_3_3_)    \n",
    "  - 5.3. [Bonus: Filtering in the spatial and spectral domain.](#toc5_3_)    \n",
    "    - 5.3.1. [Methods and Implementation](#toc5_3_1_)    \n",
    "      - 5.3.1.1. [Soft Thresholding](#toc5_3_1_1_)    \n",
    "      - 5.3.1.2. [Wavelet Filtering for 1D (Audio)](#toc5_3_1_2_)    \n",
    "      - 5.3.1.3. [Wavelet Filtering for 2D (Images)](#toc5_3_1_3_)    \n",
    "    - 5.3.2. [Interactive Widgets](#toc5_3_2_)    \n",
    "    - 5.3.3. [Metrics for Quality Assessment](#toc5_3_3_)    \n",
    "    - 5.3.4. [Data](#toc5_3_4_)    \n",
    "    - 5.3.5. [Interactive Version](#toc5_3_5_)    \n",
    "      - 5.3.5.1. [Reflection and Analysis on Wavelet-Based Filtering](#toc5_3_5_1_)    \n",
    "        - 5.3.5.1.1. [Overview of Results](#toc5_3_5_1_1_)    \n",
    "        - 5.3.5.1.2. [Haar Wavelet Justification](#toc5_3_5_1_2_)    \n",
    "        - 5.3.5.1.3. [Interpretation of Metrics](#toc5_3_5_1_3_)    \n",
    "        - 5.3.5.1.4. [Conclusions and Future Directions](#toc5_3_5_1_4_)    \n",
    "  - 5.4. [Algorithms for the detection of structures in images](#toc5_4_)    \n",
    "    - 5.4.1. [Line Detection Objective](#toc5_4_1_)    \n",
    "      - 5.4.1.1. [Methods](#toc5_4_1_1_)    \n",
    "      - 5.4.1.2. [Parameters](#toc5_4_1_2_)    \n",
    "      - 5.4.1.3. [Data](#toc5_4_1_3_)    \n",
    "      - 5.4.1.4. [Reflection and Analysis on Line Detection Results](#toc5_4_1_4_)    \n",
    "        - 5.4.1.4.1. [Results Overview](#toc5_4_1_4_1_)    \n",
    "        - 5.4.1.4.2. [Methods Effectiveness](#toc5_4_1_4_2_)    \n",
    "        - 5.4.1.4.3. [Algorithm Performance](#toc5_4_1_4_3_)    \n",
    "        - 5.4.1.4.4. [Limitations and Areas for Improvement](#toc5_4_1_4_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. <a id='toc1_'></a>[Image and Signal Processing: A Journey Through Korea](#toc0_)\n",
    "\n",
    "In this project, we will explore image and signal processing techniques, focusing on Korea. The country, known for its rich history, culture, and technological advancements, also faces challenges such as urban air pollution. Through various exercises, we will try to solve real-world problems using image and signal processing methods.\n",
    "\n",
    " **DISCLAIMER** \n",
    "ChatGPT was used to complete this notebook, for one to reflect on the task, help with code, provide ideas and ensure that the pretty printing is done properly (e.g. Latex) \n",
    "\n",
    "If there are any other sources for information, code or resources they will be added in the respective part as a link or similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. <a id='toc2_'></a>[Metrics and Tools](#toc0_)\n",
    "In this chapter all metrics and tools (filters etc.) used in the experiments are explained in great detail. They will be referenced in the experiments and the results and analysis chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1. <a id='toc2_1_'></a>[PSNR (Peak Signal-to-Noise Ratio)](#toc0_)\n",
    "\n",
    "PSNR, or Peak Signal-to-Noise Ratio, is a commonly used metric for measuring the quality of reconstructed or processed images. It quantifies the difference between the original and the processed image, effectively measuring the quality of the latter.\n",
    "\n",
    "### 2.1.1. <a id='toc2_1_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The formula to calculate PSNR is given by:\n",
    "\n",
    "$$ \\text{PSNR} = 20 \\cdot \\log_{10} \\left( \\frac{MAX_{I}}{\\sqrt{MSE}} \\right) $$\n",
    "\n",
    "Where:\n",
    "- $ MAX_{I} $ is the maximum possible pixel value of the image. For an 8-bit grayscale image, this would be 255.\n",
    "- $ MSE $ is the Mean Squared Error between the original and the processed image.\n",
    "\n",
    "Higher PSNR values generally indicate better quality, although it's worth noting that the human visual system does not perceive image quality linearly with PSNR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[SSIM (Structural Similarity Index)](#toc0_)\n",
    "\n",
    "SSIM, or Structural Similarity Index, is a metric used to measure the similarity between two images. Unlike PSNR, which focuses solely on the pixel-level errors, SSIM aims to model the perceived changes in the structural information of the image, which is more aligned with the human visual system's perception of image quality.\n",
    "\n",
    "### 2.2.1. <a id='toc2_2_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The formula to calculate SSIM is a bit more complex and is given by:\n",
    "\n",
    "$$ \\text{SSIM}(x, y) = \\frac{(2 \\mu_x \\mu_y + C_1)(2 \\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} $$\n",
    "\n",
    "Where:\n",
    "- $ \\mu_x $ and $ \\mu_y $ are the average of images $ x $ and $ y $ respectively.\n",
    "- $ \\sigma_x^2 $ and $ \\sigma_y^2 $ are the variances of $ x $ and $ y $ respectively.\n",
    "- $ \\sigma_{xy} $ is the covariance of $ x $ and $ y $.\n",
    "- $ C_1 $ and $ C_2 $ are constants to avoid division by zero.\n",
    "\n",
    "Higher SSIM values (closer to 1) indicate more similarity between the two images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. <a id='toc2_3_'></a>[Histogram Analysis](#toc0_)\n",
    "\n",
    "Histogram Analysis refers to the method of analyzing the distribution of pixel intensities in an image. A histogram is a graphical representation that shows the frequency of pixel intensities ranging from the darkest to the lightest.\n",
    "\n",
    "### 2.3.1. <a id='toc2_3_1_'></a>[Interpretation](#toc0_)\n",
    "\n",
    "A histogram can reveal important characteristics about an image:\n",
    "- A left-skewed histogram indicates that the image is generally dark.\n",
    "- A right-skewed histogram indicates a generally bright image.\n",
    "- A flat histogram is often desirable as it indicates that all intensities are equally represented, maximizing contrast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. <a id='toc2_4_'></a>[Histogram Equalization](#toc0_)\n",
    "\n",
    "Histogram Equalization is an image processing technique used to improve contrast in images. It spreads out the most frequent intensity values in an image, making it easier to identify features and objects.\n",
    "\n",
    "### 2.4.1. <a id='toc2_4_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The formula for histogram equalization is:\n",
    "\n",
    "$$ s = T(r) = (L-1) \\int_{0}^{r} p_r(w) dw $$\n",
    "\n",
    "Where:\n",
    "- $ s $ and $ r $ are the output and input pixel values, respectively.\n",
    "- $ L $ is the number of gray levels.\n",
    "- $ p_r(w) $ is the probability density function of the image's pixel intensities.\n",
    "\n",
    "This technique is especially useful in biomedical image analysis, satellite imaging, and various other fields where image clarity is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. <a id='toc2_5_'></a>[Median Filter](#toc0_)\n",
    "\n",
    "The Median Filter is a non-linear filtering technique often used in image processing to reduce \"salt-and-pepper\" noise. Unlike linear filters (e.g., average filter), which take the average of neighboring pixels, the median filter considers the median of the pixel values in a neighborhood around each pixel.\n",
    "\n",
    "### 2.5.1. <a id='toc2_5_1_'></a>[Method](#toc0_)\n",
    "\n",
    "For each pixel, the algorithm calculates the median of the pixel values in its $ n \\times n $ neighborhood and then replaces the pixel's value with the median value. \n",
    "\n",
    "The Median Filter preserves edges while effectively reducing noise, making it particularly useful for 'salt-and-pepper' noise removal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. <a id='toc2_6_'></a>[Bilateral Filter](#toc0_)\n",
    "\n",
    "The Bilateral Filter is an advanced filtering technique that aims to reduce noise while preserving edges. Unlike other smoothing filters that reduce noise but also blur edges, bilateral filtering performs a weighted average where the weights depend not only on the Euclidean distance of pixels but also on the radiometric differences (e.g., range differences like color intensity).\n",
    "\n",
    "### 2.6.1. <a id='toc2_6_1_'></a>[Method](#toc0_)\n",
    "\n",
    "The filtering process involves two components: the spatial component and the intensity component. The weight for each neighboring pixel is calculated based on these two components, ensuring that only those pixels with similar intensity to the central pixel are included in the computation of the new pixel value.\n",
    "\n",
    "Bilateral Filtering is particularly effective for edge-preserving denoising but can be computationally intensive.\n",
    "\n",
    "Diameter (d): This parameter defines the diameter of the neighborhood in which the filter will look for pixels to compute the weighted average. If the diameter is large, the algorithm will consider a larger number of pixels, which can result in more blurring but potentially better noise reduction. It's a crucial parameter that determines the spatial reach of the filter.\n",
    "\n",
    "SigmaColor: This parameter is related to color space filtering. It defines the range of intensity values that will be mixed together. A larger sigmaColor allows more colors to be averaged, resulting in more smoothing but also more blurring of color boundaries.\n",
    "\n",
    "SigmaSpace: This parameter defines how far pixels will influence each other based on their spatial closeness. Like sigmaColor, a larger sigmaSpace results in more pixels being averaged, but it can also lead to blurring of edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. <a id='toc2_7_'></a>[CLAHE (Contrast Limited Adaptive Histogram Equalization)](#toc0_)\n",
    "\n",
    "CLAHE stands for Contrast Limited Adaptive Histogram Equalization. It is an advanced variant of histogram equalization that aims to improve upon its predecessor by overcoming the issue of excessive contrast. Unlike traditional histogram equalization, which applies a single, global mapping function to the entire image, CLAHE operates on small, localized regions of the image. This makes it highly effective in improving local contrast and enhancing the definition of edges and other structures.\n",
    "\n",
    "### 2.7.1. <a id='toc2_7_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The basic idea behind CLAHE is to divide the image into a grid of small, non-overlapping blocks and perform histogram equalization on each one. The pixels in the boundaries between blocks are then interpolated to remove artifacts. The \"Contrast Limited\" part comes into play when the histogram equalization is modified to limit the amplification of noise.\n",
    "\n",
    "$$ Y = T(X) $$\n",
    "\n",
    "Where:\n",
    "- $ Y $ is the processed image\n",
    "- $ X $ is the original image\n",
    "- $ T $ is the mapping function, which varies from one block to another\n",
    "\n",
    "The effectiveness of CLAHE depends on the parameters such as block size and the limit for contrast amplification, which can be fine-tuned based on the specific needs of the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. <a id='toc2_8_'></a>[Lab Color Space Equalization](#toc0_)\n",
    "\n",
    "The Lab color space is a color-opponent space with dimensions $ L^* $ for lightness and $ a^* $ and $ b^* $ for the color-opponent dimensions. The Lab color space is designed to be perceptually uniform, meaning that the perceptual difference between colors is consistent across the color space.\n",
    "\n",
    "### 2.8.1. <a id='toc2_8_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "To enhance contrast while preserving color, one approach is to convert the image from the RGB color space to Lab color space, and then apply histogram equalization to the $ L^* $ channel only. After that, the image is converted back to RGB.\n",
    "\n",
    "1. Convert RGB to Lab: $ \\text{Lab} = f(\\text{RGB}) $\n",
    "2. Equalize $ L^* $ channel: $ L^*_{\\text{eq}} = T(L^*) $\n",
    "3. Convert Lab to RGB: $ \\text{RGB}_{\\text{eq}} = f^{-1}(\\text{Lab}_{\\text{eq}}) $\n",
    "\n",
    "Where:\n",
    "- $ L^*_{\\text{eq}} $ is the equalized $ L^* $ channel\n",
    "- $ \\text{RGB}_{\\text{eq}} $ is the final, equalized image in RGB\n",
    "- $ T $ is the histogram equalization function\n",
    "- $ f $ and $ f^{-1} $ are the conversion functions between RGB and Lab\n",
    "\n",
    "This method allows for the contrast to be enhanced without affecting the color information in the $ a^* $ and $ b^* $ channels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. <a id='toc2_9_'></a>[Unsharp Mask](#toc0_)\n",
    "\n",
    "Unsharp Mask is a digital image processing method used to enhance the sharpness of edges and details in an image. It works by subtracting a blurred version of the image from the original and adding the resulting 'mask' back to the original image.\n",
    "\n",
    "### 2.9.1. <a id='toc2_9_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The formula for Unsharp Masking can be represented as:\n",
    "\n",
    "$$ \\text{Enhanced Image} = \\text{Original Image} + \\alpha \\times (\\text{Original Image} - \\text{Blurred Image}) $$\n",
    "\n",
    "Where:\n",
    "- $ \\text{Enhanced Image} $ is the output image after applying the Unsharp Mask.\n",
    "- $ \\text{Original Image} $ is the input image that needs to be sharpened.\n",
    "- $ \\text{Blurred Image} $ is a Gaussian-blurred version of the original image.\n",
    "- $ \\alpha $ is a scaling factor that controls how much of the unsharp mask is added to the original image.\n",
    "\n",
    "This technique is commonly used in digital photography, medical imaging, and computer graphics to improve the visibility of edges and fine details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. <a id='toc2_10_'></a>[Laplacian of Gaussian (LoG) Filter](#toc0_)\n",
    "\n",
    "Laplacian of Gaussian (LoG) is a two-step process that involves smoothing an image with a Gaussian filter, then applying the Laplacian filter. The LoG operator takes the second derivative of the image to find regions where there is a rapid change in intensity. The Gaussian filter is used for smoothing and noise reduction ahead of this edge detection.\n",
    "\n",
    "### 2.10.1. <a id='toc2_10_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The mathematical expression for the Laplacian of Gaussian $ \\nabla^2 G(x, y) $ for a 2D function $ f(x, y) $ is:\n",
    "\n",
    "$$\n",
    "\\nabla^2 G(x, y) = \\frac{\\partial^2 G}{\\partial x^2} + \\frac{\\partial^2 G}{\\partial y^2}\n",
    "$$\n",
    "\n",
    "Where $ G(x, y) $ is the Gaussian function:\n",
    "\n",
    "$$\n",
    "G(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "After smoothing the image $ f(x, y) $ with a Gaussian filter, the Laplacian $ \\nabla^2 G(x, y) $ is applied. The resulting image can be given as:\n",
    "\n",
    "$$\n",
    "\\text{LoG Image} = \\nabla^2 (G(x, y) * f(x, y))\n",
    "$$\n",
    "\n",
    "To enhance the image using LoG, the processed image can be added back to the original image:\n",
    "\n",
    "$$\n",
    "\\text{Enhanced Image} = \\text{Original Image} + \\alpha \\times (\\text{LoG Image})\n",
    "$$\n",
    "\n",
    "Here $ \\alpha $ is a scaling factor to control the extent of sharpening.\n",
    "\n",
    "### 2.10.2. <a id='toc2_10_2_'></a>[Kernel](#toc0_)\n",
    "\n",
    "In practice, the LoG operation can be approximated by a convolution with a kernel. The kernel encapsulates both the Gaussian smoothing and the Laplacian for edge detection. The size and values in the kernel are determined by the standard deviation $ \\sigma $ of the Gaussian function.\n",
    "\n",
    "### 2.10.3. <a id='toc2_10_3_'></a>[Applications](#toc0_)\n",
    "\n",
    "The Laplacian of Gaussian method is commonly used for edge detection and texture discrimination. It is particularly effective when you need to distinguish edge points from noise. It is widely employed in fields like medical imaging, computer vision, and machine learning for feature extraction and image segmentation.\n",
    "\n",
    "By combining Gaussian blurring with Laplacian for edge detection, the LoG filter is more robust to noise, making it a sensible choice for real-world applications where image quality may vary.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. <a id='toc2_11_'></a>[Signal-to-Noise Ratio (SNR)](#toc0_)\n",
    "\n",
    "Signal-to-Noise Ratio (SNR) is a metric used to quantify the quality of an audio signal, particularly in the context of separating vocals from the musical background. It evaluates the clarity of the isolated vocals by comparing the power of the separated background music to the noise introduced by the separation process.\n",
    "\n",
    "### 2.11.1. <a id='toc2_11_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The SNR is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "SNR (dB) = 10 \\times \\log_{10}\\left(\\frac{\\text{mean}(Signal^2)}{\\text{mean}(Noise^2)}\\right)\n",
    "$$\n",
    "\n",
    "### 2.11.2. <a id='toc2_11_2_'></a>[Applications](#toc0_)\n",
    "\n",
    "The SNR is commonly used in audio processing to evaluate the effectiveness of algorithms in separating audio components. A higher SNR value indicates less noise and a better separation quality. Therefore, it is crucial for assessing the effectiveness of the REPET-SIM method in isolating vocals from music.\n",
    "\n",
    "\n",
    "## 2.12. <a id='toc2_12_'></a>[Spectral Contrast](#toc0_)\n",
    "\n",
    "Spectral contrast is a feature used in audio signal processing and music information retrieval. It provides insights into how distinct the vocal and musical components are in the frequency domain after separation. Spectral contrast measures the difference in amplitude between peaks and valleys in the spectrum for each frequency band.\n",
    "\n",
    "### 2.12.1. <a id='toc2_12_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The spectral contrast $C$ for each frequency band is calculated as:\n",
    "\n",
    "$$\n",
    "C = \\frac{F_{\\text{peak}} - F_{\\text{valley}}}{F_{\\text{peak}} + F_{\\text{valley}}}\n",
    "$$\n",
    "\n",
    "### 2.12.2. <a id='toc2_12_2_'></a>[Applications](#toc0_)\n",
    "\n",
    "Spectral contrast is often used in audio classification tasks and music information retrieval. It's a reliable metric for assessing how well an algorithm can distinguish between different types of audio components—such as vocals and musical instruments—especially after a separation process like REPET-SIM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13. <a id='toc2_13_'></a>[Harmonic-to-Noise Ratio (HNR)](#toc0_)\n",
    "\n",
    "Harmonic-to-Noise Ratio (HNR) is a measure used in audio signal analysis to assess the proportion of harmonic (periodic) sound to noise (aperiodic) sound within an audio signal. It's a key indicator of signal quality, especially in vocal recordings, where a higher ratio suggests a cleaner, more harmonic-rich sound, while a lower ratio indicates more noise and potential distortion.\n",
    "\n",
    "### 2.13.1. <a id='toc2_13_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The HNR can be calculated using the following formula:\n",
    "\n",
    "$$\n",
    "HNR (dB) = 10 \\times \\log_{10}\\left(\\frac{\\text{Power of Harmonic Components}}{\\text{Power of Noise Components}}\\right)\n",
    "$$\n",
    "\n",
    "The power of harmonic components and noise components is typically obtained through signal processing techniques that separate the harmonic content from the noise content in a sound signal.\n",
    "\n",
    "### 2.13.2. <a id='toc2_13_2_'></a>[Applications](#toc0_)\n",
    "\n",
    "HNR is particularly useful in voice analysis, where it serves as an objective metric to evaluate the quality of a voice signal or the efficiency of voice enhancement algorithms. High HNR values are often associated with clear, intelligible speech, and low values may indicate a hoarse or breathy voice quality. This makes HNR a valuable tool in various fields, such as speech therapy, singing voice analysis, and telecommunications, where vocal clarity is of paramount importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.14. <a id='toc2_14_'></a>[REPET-SIM Method](#toc0_)\n",
    "\n",
    "Sources:\n",
    "https://github.com/zafarrafii/REPET-Python\n",
    "https://ieeexplore.ieee.org/abstract/document/6637768\n",
    "\n",
    "### 2.14.1. <a id='toc2_14_1_'></a>[Overview](#toc0_)\n",
    "The REPET-SIM method is a sophisticated audio processing technique designed for separating vocals from the musical background in audio tracks. It works by transforming the audio signal into the frequency domain and applying various types of filtering and masking to isolate different components of the audio.\n",
    "\n",
    "### 2.14.2. <a id='toc2_14_2_'></a>[Short-Time Fourier Transform (STFT)](#toc0_)\n",
    "The Short-Time Fourier Transform (STFT) is an algorithm that computes the Discrete Fourier Transform (DFT) and its inverse quickly. The STFT is a critical tool in digital signal processing, allowing for the analysis of the frequency components within a signal. The DFT is expressed mathematically as:\n",
    "\n",
    "$$\n",
    "X(k) = \\sum_{n=0}^{N-1} x(n) \\exp\\left(-j\\frac{2\\pi kn}{N}\\right)\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "- $ x(n) $ is the original time-domain signal.\n",
    "- $ N $ is the total number of samples in the signal.\n",
    "- $ k $ is the index of the frequency component.\n",
    "- $ X(k) $ represents the frequency domain representation of the signal.\n",
    "\n",
    "The STFT reduces the number of computations needed from $$ O(N^2) $$ for the DFT to $$ O(N \\log N) $$, which is particularly advantageous for signals with a large number of samples. The STFT is used in a wide array of applications such as audio processing, image analysis, and communications systems.\n",
    "\n",
    "### 2.14.3. <a id='toc2_14_3_'></a>[Non-local Filtering and Soft Mask](#toc0_)\n",
    "Based on the STFT, non-local filtering is applied to create a soft mask for isolating vocals. The soft mask is constructed using a similarity metric, often cosine similarity, and median aggregation over a specified time window. This soft mask is then applied to the original STFT to separate the components. \n",
    "\n",
    "### 2.14.4. <a id='toc2_14_4_'></a>[Soft-masking to Isolate Vocals and Music](#toc0_)\n",
    "Soft-masking is the final step. It takes the soft mask and applies it to the original STFT of the audio signal to produce separate frequency representations for vocals and music. These are then converted back to the time domain to produce separate audio tracks. Mathematically, the soft mask $M$ applied to the original STFT $X$ yields the isolated components $Y$ and $Z$ as:\n",
    "$$\n",
    "Y = M \\cdot X\n",
    "$$\n",
    "$$\n",
    "Z = (1 - M) \\cdot X\n",
    "$$\n",
    "Here, $Y$ could represent the vocals, and $Z$ could represent the music or vice versa.\n",
    "\n",
    "### 2.14.5. <a id='toc2_14_5_'></a>[Applications](#toc0_)\n",
    "The REPET-SIM method is highly effective for tasks like creating karaoke versions of songs by isolating vocals from the background music. Its application is significant in the context of entertainment and audio engineering, offering a more enjoyable and customizable audio experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.15. <a id='toc2_15_'></a>[Laplacian Kernel](#toc0_)\n",
    "\n",
    "The Laplacian Kernel is used in image processing to emphasize regions of rapid intensity change and is therefore often used for edge detection. The kernel is a square matrix that represents the second-order derivative operation applied to an image. It highlights regions where the intensity of pixels changes abruptly, which usually correspond to the edges in an image.\n",
    "\n",
    "### 2.15.1. <a id='toc2_15_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The Laplacian Kernel can take various forms, one of the common ones is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is convolved with the image to produce the effect of highlighting edges. The center value $-4$ is indicative of the high-pass characteristics of the filter, amplifying high-frequency components that correspond to edges.\n",
    "\n",
    "## 2.16. <a id='toc2_16_'></a>[Gaussian Kernel](#toc0_)\n",
    "\n",
    "The Gaussian Kernel is a type of image filter that is used to create a blurring effect. This kernel is based on the Gaussian (normal) distribution, where values farther from the center of the kernel have smaller weights and thus lesser influence on the convolution output.\n",
    "\n",
    "### 2.16.1. <a id='toc2_16_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "A Gaussian Kernel can be defined as a matrix used for blurring, smoothing, and reducing detail and noise in images. A 3x3 example would be:\n",
    "\n",
    "$$\n",
    "\\frac{1}{16} \\times\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 4 & 2 \\\\\n",
    "1 & 2 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The values are calculated using the Gaussian function, and the size of the matrix can vary depending on the desired amount of blurring.\n",
    "\n",
    "## 2.17. <a id='toc2_17_'></a>[Canny Edge Detection](#toc0_)\n",
    "\n",
    "Canny Edge Detection is a multi-stage algorithm used to detect a wide range of edges in images. It is known for its ability to detect weak edges and for providing good localization of the detected edges.\n",
    "\n",
    "### 2.17.1. <a id='toc2_17_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The Canny edge detection algorithm involves several steps:\n",
    "1. Apply Gaussian blur to smooth the image.\n",
    "2. Find the intensity gradients of the image.\n",
    "3. Apply non-maximum suppression to get rid of spurious response to edge detection.\n",
    "4. Apply double threshold to determine potential edges.\n",
    "5. Track edge by hysteresis: finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
    "\n",
    "## 2.18. <a id='toc2_18_'></a>[Probabilistic Hough Line Transform](#toc0_)\n",
    "\n",
    "Probabilistic Hough Line Transform is a variant of the Hough Transform which is used to detect lines in an image. It considers only a random subset of points that are sufficient for line detection, making it more efficient than the standard Hough Transform.\n",
    "\n",
    "### 2.18.1. <a id='toc2_18_1_'></a>[Formula](#toc0_)\n",
    "\n",
    "The algorithm can be summarized as follows:\n",
    "1. Edges in the image are detected, typically using the Canny edge detector.\n",
    "2. A subset of points from the detected edges is randomly selected.\n",
    "3. These points are mapped to the Hough space and analyzed for potential lines.\n",
    "4. Lines are determined based on the accumulation of points that form a line within a certain threshold.\n",
    "\n",
    "The parameters used in the transform are:\n",
    "- Threshold: The minimum number of intersections to “detect” a line.\n",
    "- minLineLength: The minimum number of points that can form a line. Lines with fewer points are disregarded.\n",
    "- maxLineGap: The maximum gap between two points to be considered in the same line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Helper Functions](#toc0_)\n",
    "To help me with the experiments, I created a few helper functions. They are explained in detail below and referenced in the experiments and the results and analysis chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.fft import fft, ifft\n",
    "from IPython.display import display, Audio\n",
    "import time\n",
    "from skimage.filters import unsharp_mask, laplace, gaussian\n",
    "from skimage.util import img_as_ubyte\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "from scipy.spatial import distance\n",
    "from scipy.signal import welch\n",
    "from numpy.fft import fft2\n",
    "import pywt\n",
    "from ipywidgets import FloatSlider, interactive\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "def calculate_psnr(original_img, compared_img):\n",
    "    psnr_value = cv2.PSNR(original_img, compared_img)\n",
    "    return psnr_value\n",
    "\n",
    "def calculate_mse(original, processed):\n",
    "    return np.mean((original - processed) ** 2)\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"\n",
    "    Calculate the Structural Similarity Index (SSIM) between two images.\n",
    "    \"\"\"\n",
    "    if len(img1.shape) == 3:\n",
    "        img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img1_gray = img1\n",
    "\n",
    "    if len(img2.shape) == 3:\n",
    "        img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img2_gray = img2\n",
    "\n",
    "    return ssim(img1_gray, img2_gray)\n",
    "\n",
    "def plot_image_and_histogram_separated(image, title='Image'):\n",
    "    \"\"\"\n",
    "    Displays the image along with histograms for its color channels using GridSpec for layout.\n",
    "    \n",
    "    Parameters:\n",
    "        image: The input image in BGR format.\n",
    "    \"\"\"\n",
    "\n",
    "    channels = cv2.split(image)\n",
    "    color = ('b', 'g', 'r')\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 12))\n",
    "\n",
    "    # GridSpec layout with height_ratios to dynamically adjust the first row's height\n",
    "    gs = GridSpec(2, 3, figure=fig, height_ratios=[3, 1])\n",
    "\n",
    "    # Image occupies the first row and spans all columns\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title(title, fontsize=18)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Histograms occupy the second row, each in one column\n",
    "    for i, col in enumerate(color):\n",
    "        ax2 = fig.add_subplot(gs[1, i])\n",
    "        ax2.hist(channels[i].ravel(), 256, [0, 256], color=col, alpha=0.7, label=f'{col.upper()} channel')\n",
    "        ax2.set_title(f'{col.upper()} channel Histogram', fontsize=16)\n",
    "        ax2.set_xlim([0, 256])\n",
    "        ax2.ticklabel_format(useOffset=False)  # Disable scientific notation\n",
    "        ax2.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.set_xlabel('Pixel Intensity')\n",
    "        if i == 0:\n",
    "            ax2.set_ylabel('Pixel Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_before_after(image1, image2, title1='Before', title2='After', greyscale=False):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    if greyscale:\n",
    "        axs[0, 0].imshow(image1, cmap='gray')\n",
    "    else:\n",
    "        axs[0, 0].imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 0].set_title(title1, fontsize=16)\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "    if greyscale:\n",
    "        axs[0, 1].imshow(image2, cmap='gray')\n",
    "    else:\n",
    "        axs[0, 1].imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 1].set_title(title2, fontsize=16)\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    if greyscale:\n",
    "        axs[1, 0].hist(image1.ravel(), 256, [0, 256], color='k')\n",
    "    else:\n",
    "        axs[1, 0].hist(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY).ravel(), 256, [0, 256], color='k')\n",
    "    axs[1, 0].set_title(f'{title1} Histogram', fontsize=16)\n",
    "    axs[1, 0].set_xlim([0, 256])\n",
    "    axs[1, 0].set_xlabel('Pixel Intensity')\n",
    "    axs[1, 0].set_ylabel('Pixel Count')\n",
    "\n",
    "    if greyscale:\n",
    "        axs[1, 1].hist(image2.ravel(), 256, [0, 256], color='k')\n",
    "    else:\n",
    "        axs[1, 1].hist(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY).ravel(), 256, [0, 256], color='k')\n",
    "    axs[1, 1].set_title(f'Histogram {title2} ', fontsize=16)\n",
    "    axs[1, 1].set_xlim([0, 256])\n",
    "    axs[1, 1].set_xlabel('Pixel Intensity')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_comparison_metrics(images_dict, technique_dict, metric_funcs, include_histogram=False):\n",
    "    \"\"\"\n",
    "    Plot a comparison of images processed with various techniques alongside calculated metrics.\n",
    "\n",
    "    Args:\n",
    "    - images_dict (dict): A dictionary with keys as image names and values as original image arrays.\n",
    "    - technique_dict (dict): A dictionary with keys as technique names and values as corresponding functions.\n",
    "    - metric_funcs (dict): A dictionary with keys as metric names and values as corresponding metric functions.\n",
    "    - include_histogram (bool, optional): Whether to include histograms in the plot. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing calculated metrics.\n",
    "    - dict: A dictionary containing processed images.\n",
    "    \"\"\"\n",
    "    metrics_values = {name: {} for name in metric_funcs.keys()}\n",
    "    processed_images_dict = {}\n",
    "\n",
    "    for img_name, original_image in images_dict.items():\n",
    "        subplot_rows = 2 if include_histogram else 1\n",
    "        fig, axs = plt.subplots(subplot_rows, len(technique_dict) + 1, figsize=(20, 10))\n",
    "        axs = axs.T if subplot_rows == 1 else axs.T.tolist()\n",
    "\n",
    "        axs[0][0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        axs[0][0].set_title(f'{img_name} Original', fontsize=16)\n",
    "        axs[0][0].axis('off')\n",
    "\n",
    "        processed_images_dict[img_name] = {}\n",
    "\n",
    "        if include_histogram:\n",
    "            axs[0][1].hist(cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY).ravel(), 256, [0, 256], color='k')\n",
    "            axs[0][1].set_title(f'{img_name} Original Histogram', fontsize=16)\n",
    "            axs[0][1].set_xlabel('Pixel Intensity')\n",
    "            axs[0][1].set_ylabel('Pixel Count')\n",
    "\n",
    "        for i, (tech_name, func) in enumerate(technique_dict.items()):\n",
    "            processed_image = func(original_image)\n",
    "            processed_images_dict[img_name][tech_name] = processed_image\n",
    "\n",
    "            metric_text = ''\n",
    "            for metric_name, metric_func in metric_funcs.items():\n",
    "                metric_val = metric_func(original_image, processed_image)\n",
    "                metrics_values[metric_name].setdefault(tech_name, {})[img_name] = metric_val\n",
    "                metric_text += f'{metric_name}: {metric_val:.4f}\\n'\n",
    "\n",
    "            axs[i + 1][0].imshow(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))\n",
    "            axs[i + 1][0].set_title(f'{tech_name}\\n{metric_text}', fontsize=16)\n",
    "            axs[i + 1][0].axis('off')\n",
    "\n",
    "            if include_histogram:\n",
    "                axs[i + 1][1].hist(cv2.cvtColor(processed_image, cv2.COLOR_BGR2GRAY).ravel(), 256, [0, 256], color='k')\n",
    "                axs[i + 1][1].set_title(f'{tech_name} Histogram', fontsize=16)\n",
    "                axs[i + 1][1].set_xlabel('Pixel Intensity')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return metrics_values, processed_images_dict\n",
    "\n",
    "def plot_audio_waveform(audio, sr, title, normalize=True):\n",
    "    \"\"\"\n",
    "    Display the audio waveform using matplotlib and include a play button.\n",
    "    \"\"\"\n",
    "    # Display audio player\n",
    "    display(Audio(audio, rate=sr, normalize=normalize))\n",
    "\n",
    "    # Display waveform\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    time_axis = np.linspace(0, len(audio) / sr, num=len(audio))\n",
    "    plt.plot(time_axis, audio)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. <a id='toc4_'></a>[Basics Image and Signal Processing (LE1)](#toc0_)\n",
    "\n",
    "## 4.1. <a id='toc4_1_'></a>[Image Properties](#toc0_)\n",
    "\n",
    "Find various images of different scenes suitable for your country or take some yourself. The images should be suitable for demonstrating adjustments to the image properties {'noise, contrast, sharpness'} in experiments. To do this, first measure the image properties assigned to you on your images using appropriate metrics. Then define a target how the image properties assigned to you should be changed. Define some experiments to achieve this goal and perform the experiments with your images and appropriate methods. Analyze the histograms of the original images and during your experiments. Perform histogram equalization if necessary. Discuss your choice of data, parameters and methods and the results obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. <a id='toc4_1_1_'></a>[Problem](#toc0_)\n",
    "South Korea is a country with a rich history and culture, and is also known for its technological advancements. However, the country also faces challenges such as urban air pollution. While we can't fix the problem directly, we can try to improve the quality of images taken in polluted areas. In this section, we will try to improve the quality of an image taken in Seoul, South Korea, by applying various image processing techniques.\n",
    "\n",
    "Here we see the original image of Namsan Tower, Seoul (source: https://www.koreaherald.com/view.php?ud=20180514000715 Yonhap). The image seems to have a fog (air pollution). Air pollution is a big problem in many cities of South Korea.\n",
    "\n",
    "In order to solve this problem we will apply contrast, noise and sharpness filters to the image. The goal is to improve the image quality and make the Namsan Tower more visible, so effectively try to remove the air pollution without destroying the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_path = 'images/Namsan Tower Yonhap.jpeg'\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "plot_image_and_histogram_separated(image, title='Namsan Tower Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2. <a id='toc4_1_2_'></a>[Experiment 1: Contrast Enhancement Through Various Techniques](#toc0_)\n",
    "\n",
    "#### 4.1.2.1. <a id='toc4_1_2_1_'></a>[Objective](#toc0_)\n",
    "The primary goal of this experiment is to enhance the contrast of the image of Namsan Tower, Seoul, while preserving its color. Due to the haze likely caused by air pollution, visibility is poor. The aim is to make the tower and other structures in the image more discernible without introducing excessive noise or distortions.\n",
    "\n",
    "#### 4.1.2.2. <a id='toc4_1_2_2_'></a>[Approach](#toc0_)\n",
    "We will explore various techniques for contrast enhancement, including:\n",
    "\n",
    "1. **Histogram Equalization**: A well-known method for redistributing pixel intensities.\n",
    "2. **CLAHE (Contrast Limited Adaptive Histogram Equalization)**: A more localized approach to histogram equalization.\n",
    "3. **Lab Color Space Equalization**: Applies histogram equalization to the Lightness channel to preserve color.\n",
    "\n",
    "Each method has its own set of strengths and weaknesses, which will be assessed in terms of their impact on image quality.\n",
    "\n",
    "#### 4.1.2.3. <a id='toc4_1_2_3_'></a>[Metrics and Tools](#toc0_)\n",
    "\n",
    "##### 4.1.2.3.1. <a id='toc4_1_2_3_1_'></a>[PSNR (Peak Signal-to-Noise Ratio)](#toc0_)\n",
    "PSNR will serve as our quantitative metric to assess image quality. Lower PSNR values indicate more noise introduced by the contrast enhancement technique, allowing us to strike a balance between noise and visibility.\n",
    "\n",
    "##### 4.1.2.3.2. <a id='toc4_1_2_3_2_'></a>[Histogram Analysis](#toc0_)\n",
    "Histograms will serve as a supplementary tool for qualitative inspection, allowing us to visualize the distribution of pixel intensities across different color channels.\n",
    "\n",
    "By employing both PSNR and Histogram Analysis, we aim to conduct a comprehensive evaluation of the processed image.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def histogram_equalization(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    equalized_gray_image = cv2.equalizeHist(gray_image)\n",
    "    return cv2.cvtColor(equalized_gray_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def clahe(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    equalized_gray_image = clahe.apply(gray_image)\n",
    "    return cv2.cvtColor(equalized_gray_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def lab_color_space(image):\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "    l, a, b = cv2.split(lab_image)\n",
    "    equalized_l = cv2.equalizeHist(l)\n",
    "    equalized_lab_image = cv2.merge([equalized_l, a, b])\n",
    "    return cv2.cvtColor(equalized_lab_image, cv2.COLOR_Lab2BGR)\n",
    "\n",
    "contrast_techniques = {\n",
    "    'Histogram Equalization': histogram_equalization,\n",
    "    'CLAHE': clahe,\n",
    "    'Lab Color Space': lab_color_space\n",
    "}\n",
    "\n",
    "images = {\n",
    "    'Namsan Tower': image\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'PSNR': calculate_psnr,\n",
    "    'SSIM': calculate_ssim\n",
    "}\n",
    "\n",
    "metrics_result, processed_images = plot_comparison_metrics(images, contrast_techniques, metrics, include_histogram=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2.3.3. <a id='toc4_1_2_3_3_'></a>[Results and Analysis](#toc0_)\n",
    "\n",
    "The experiment aimed to evaluate the effectiveness of three contrast enhancement techniques—Histogram Equalization, CLAHE, and Lab Color Space Equalization—in improving the visibility of structures in an image of Namsan Tower shrouded by air pollution. The results were rather revealing, raising questions about the metrics traditionally used to evaluate image quality.\n",
    "\n",
    "**Histogram Equalization** yielded a significant improvement in visibility, effectively 'cutting through' the haze that initially veiled the tower. However, the conversion to grayscale was a double-edged sword—while the haze was, in essence, a color distortion, the loss of color information might not be acceptable in all application contexts. Moreover, the technique introduced what appears to be Gaussian noise, corroborated by a low PSNR value of 12.79 dB.\n",
    "\n",
    "**CLAHE**, despite having the highest PSNR value of 27.73 dB, fell short of expectations. The technique seemed to only desaturate the image without making any meaningful impact on the haze. This puts into question the reliability of PSNR as the sole metric for this specific problem—higher PSNR did not correlate with better haze removal.\n",
    "\n",
    "**Lab Color Space Equalization** offered a result quite similar to Histogram Equalization but with color preservation. This seemed like the best of both worlds until subtle fragmenting artifacts were noticed. These artifacts, although not overwhelming, could be problematic in applications requiring high fidelity. The PSNR and SSIM values were 12.65 dB and 0.6431, respectively, indicating the introduction of noise and a slight structure change in the image.\n",
    "\n",
    "The **SSIM** values also brought another layer of complexity. While CLAHE had an SSIM value of 0.9626, suggesting excellent structural preservation, it failed in our primary objective of haze removal. On the other hand, Histogram Equalization had an SSIM value of 0.6694, indicating a significant structural change, which in this context was desirable for achieving our objective.\n",
    "\n",
    "In summary, the experiment highlighted the limitations of traditional metrics like PSNR and SSIM in evaluating image enhancement techniques for specialized applications like haze removal. While each technique had its merits and demerits, none provided a perfect solution, laying the groundwork for future experiments to explore hybrid techniques or parameter tuning.\n",
    "\n",
    "Continuing from this experiment, we will focus on Histogram Equalization and Lab Color Space Equalization, which showed the most promise in terms of haze removal and color preservation, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. <a id='toc4_1_3_'></a>[Experiment 2: Comparative Noise Reduction on Equalized Image](#toc0_)\n",
    "\n",
    "#### 4.1.3.1. <a id='toc4_1_3_1_'></a>[Objective](#toc0_)\n",
    "The primary goal of this experiment is to address the noise introduced by the contrast enhancement in Experiment 1. We aim to mitigate this noise while preserving the image's structure and quality.\n",
    "\n",
    "#### 4.1.3.2. <a id='toc4_1_3_2_'></a>[Approach](#toc0_)\n",
    "We will focus on two denoising techniques that have shown promise in preliminary tests: Non-Local Means and Bilateral Filtering. By applying these techniques to the equalized image from Experiment 1, we aim to identify the most effective method for noise reduction.\n",
    "\n",
    "#### 4.1.3.3. <a id='toc4_1_3_3_'></a>[Metrics and Tools](#toc0_)\n",
    "\n",
    "##### 4.1.3.3.1. <a id='toc4_1_3_3_1_'></a>[PSNR (Peak Signal-to-Noise Ratio)](#toc0_)\n",
    "PSNR remains our main quantitative metric for assessing image noise levels after each denoising technique. Higher PSNR values indicate less noise and better image quality.\n",
    "\n",
    "##### 4.1.3.3.2. <a id='toc4_1_3_3_2_'></a>[SSIM (Structural Similarity Index)](#toc0_)\n",
    "We will also use SSIM to evaluate the structural integrity of the image post-denoising.\n",
    "\n",
    "##### 4.1.3.3.3. <a id='toc4_1_3_3_3_'></a>[Histogram Analysis](#toc0_)\n",
    "Histograms will continue to be used for qualitative evaluations, allowing us to visually assess the impact of each denoising method on the image quality.\n",
    "\n",
    "#### 4.1.3.4. <a id='toc4_1_3_4_'></a>[Methods](#toc0_)\n",
    "\n",
    "1. **Non-Local Means Denoising**: Effective for removing noise while preserving structure but can be computationally intensive.\n",
    "   \n",
    "2. **Bilateral Filtering**: Preserves edges while reducing noise but is also computationally intensive.\n",
    "\n",
    "We will compute PSNR and SSIM values for each method, and analyze histograms before and after the denoising process. These evaluations will guide our approach for subsequent experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "hist_eq_image = processed_images['Namsan Tower']['Histogram Equalization']\n",
    "lab_eq_image = processed_images['Namsan Tower']['Lab Color Space']\n",
    "\n",
    "def median_filter(image, kernel_size=3):\n",
    "    return cv2.medianBlur(image, kernel_size)\n",
    "\n",
    "def bilateral_filter(image, d=30, sigmaColor=40, sigmaSpace=30):\n",
    "    return cv2.bilateralFilter(image, d, sigmaColor, sigmaSpace)\n",
    "\n",
    "images_to_denoise = {\n",
    "    'Hist Eq Image': hist_eq_image,\n",
    "    'Lab Eq Image': lab_eq_image\n",
    "}\n",
    "\n",
    "denoise_techniques = {\n",
    "    'Non-Local Means Denoising': cv2.fastNlMeansDenoisingColored,\n",
    "    'Bilateral Filtering': bilateral_filter\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'PSNR': calculate_psnr,\n",
    "    'SSIM': calculate_ssim\n",
    "}\n",
    "\n",
    "# Perform the comparison and store the metric values for future use\n",
    "metrics_result, processed_images_denoised = plot_comparison_metrics(images_to_denoise, denoise_techniques, metrics, include_histogram=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3.5. <a id='toc4_1_3_5_'></a>[Results and Analysis](#toc0_)\n",
    "\n",
    "The second experiment was conducted to assess and compare the effectiveness of two advanced denoising techniques—Non-Local Means Denoising (NLMD) and Bilateral Filtering—in mitigating the noise introduced by contrast enhancement in Experiment 1. The techniques were applied to images previously enhanced using Histogram Equalization and Lab Color Space Equalization. The primary metrics for evaluation were PSNR and SSIM, supplemented by qualitative histogram analysis.\n",
    "\n",
    "**Non-Local Means Denoising (NLMD)**  \n",
    "NLMD was chosen for its reputation in effective noise reduction while preserving structural integrity. Although it didn't significantly outperform Bilateral Filtering in terms of PSNR and SSIM, it excelled in maintaining the intricate details of structures like buildings. Its main drawback was its computational intensity, which could be a bottleneck in real-time or resource-constrained applications.\n",
    "\n",
    "**Bilateral Filtering**  \n",
    "The default parameters for Bilateral Filtering were initially too aggressive, leading to a loss of fine details. After a bit of trying around some optimal parameters (`d=30`, `sigmaColor=40`, `sigmaSpace=30`) were, the technique showed marked improvement. It effectively reduced noise while preserving structure, which was reflected in the improved PSNR and SSIM scores. Despite its successes, it introduced a certain level of blur (buildings), most noticeable when compared with NLMD.\n",
    "\n",
    "**PSNR and SSIM Metrics**  \n",
    "Both metrics were instrumental in quantitatively evaluating the denoising techniques. Bilateral Filtering showed a balanced improvement in both PSNR and SSIM after parameter tuning. However, NLMD, despite its computational cost, maintained a more consistent balance between noise reduction and structural preservation, making it the more suitable choice for this specific application.\n",
    "\n",
    "**Color Preservation vs. Structural Integrity**  \n",
    "While Lab Color Space Equalization did preserve colors, it was determined that color retention is not crucial in this context. The original image is tainted by air pollution, making the color less relevant for the analysis. NLMD, which maintained the structural details better, will be the focus moving forward.\n",
    "\n",
    "In summary, this experiment highlights the critical role of parameter tuning in the performance of denoising techniques. It also brings attention to the computational limitations of methods like NLMD. Nevertheless, NLMD's superior performance in structural preservation makes it the technique of choice for future experiments, particularly Experiment 3, where it will be applied to the equalized image.\n",
    "\n",
    "Future directions include exploring hybrid denoising techniques or employing machine learning methods for more advanced parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. <a id='toc4_1_4_'></a>[Experiment 3: Enhancing Image Sharpness](#toc0_)\n",
    "\n",
    "#### 4.1.4.1. <a id='toc4_1_4_1_'></a>[Objective](#toc0_)\n",
    "The main goal of this experiment is to enhance the sharpness of the denoised image obtained from Experiment 2. The focus is on techniques that can significantly improve the clarity and detail of the image without introducing excessive noise or artifacts.\n",
    "\n",
    "#### 4.1.4.2. <a id='toc4_1_4_2_'></a>[Approach](#toc0_)\n",
    "Given that the previous experiments involved complex methods for denoising and contrast enhancement, we aim for a more straightforward approach in this phase. We will employ two effective yet simple techniques for improving image sharpness:\n",
    "1. Unsharp Mask\n",
    "2. Laplacian of Gaussian\n",
    "\n",
    "We will apply these sharpening techniques to the image processed using Non-Local Means Denoising from Experiment 2. This allows us to directly compare and enhance an image that has already been optimized for noise reduction and detail retention.\n",
    "\n",
    "#### 4.1.4.3. <a id='toc4_1_4_3_'></a>[Metrics and Tools](#toc0_)\n",
    "\n",
    "##### 4.1.4.3.1. <a id='toc4_1_4_3_1_'></a>[PSNR (Peak Signal-to-Noise Ratio)](#toc0_)\n",
    "Traditionally used for assessing the quality of signal or image, PSNR will provide an indication of how much the sharpness techniques are affecting the overall image fidelity.\n",
    "\n",
    "##### 4.1.4.3.2. <a id='toc4_1_4_3_2_'></a>[SSIM (Structural Similarity Index)](#toc0_)\n",
    "The SSIM metric will help us understand the impact of these techniques on the structural integrity of the image, particularly in retaining important details.\n",
    "\n",
    "##### 4.1.4.3.3. <a id='toc4_1_4_3_3_'></a>[Histogram Analysis](#toc0_)\n",
    "Histograms will serve as a qualitative tool to evaluate the distribution of pixel intensities in the image. Specifically, we will focus on changes in edge intensities that signify improvements in sharpness.\n",
    "\n",
    "#### 4.1.4.4. <a id='toc4_1_4_4_'></a>[Methods](#toc0_)\n",
    "\n",
    "1. **Unsharp Mask**: This method aims to boost the sharpness by adding a scaled version of the negative Laplacian image to the original image. It's a straightforward and commonly used technique for edge enhancement.\n",
    "\n",
    "2. **Laplacian of Gaussian**: This method applies a Gaussian blur followed by a Laplacian filter. It is effective in highlighting regions of rapid intensity change and serves as an edge-enhancement technique.\n",
    "\n",
    "Both methods will be applied to the Non-Local Means Denoised image from Experiment 2. Metrics like PSNR and SSIM will be calculated to evaluate their effectiveness, and histograms will be analyzed to observe the impact on edge intensities.\n",
    "\n",
    "By focusing on these two techniques, the experiment aims to provide actionable insights into effective methods for image sharpness enhancement without adding unnecessary complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def laplacian_of_gaussian(image, sigma=2.0):\n",
    "    blurred = gaussian(image, sigma=sigma)\n",
    "    laplacian = laplace(blurred)\n",
    "    enhanced_image = image + laplacian\n",
    "    # img_as_ubyte doesn't work here \n",
    "    return np.clip(enhanced_image, 0, 255).astype('uint8')\n",
    "\n",
    "def unsharp_mask_cv2(image, radius=1.2, amount=1.3):\n",
    "    return img_as_ubyte(unsharp_mask(image, radius=radius, amount=amount))\n",
    "\n",
    "\n",
    "# Create a dictionary of images to apply sharpening techniques to\n",
    "images_to_sharpen = {\n",
    "    'Non-Local Means Denoised Image': processed_images_denoised['Hist Eq Image']['Non-Local Means Denoising'],\n",
    "}\n",
    "\n",
    "# Create a dictionary of sharpening techniques to apply\n",
    "sharpen_techniques = {\n",
    "    'Unsharp Mask': unsharp_mask_cv2,\n",
    "    'Laplacian of Gaussian': laplacian_of_gaussian\n",
    "}\n",
    "\n",
    "# Define metrics to be calculated\n",
    "metrics = {\n",
    "    'PSNR': calculate_psnr,\n",
    "    'SSIM': calculate_ssim\n",
    "}\n",
    "\n",
    "# Perform the comparison and store the metric values for future use\n",
    "metrics_result, processed_images_sharpened = plot_comparison_metrics(images_to_sharpen, sharpen_techniques, metrics, include_histogram=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4.5. <a id='toc4_1_4_5_'></a>[Results and Analysis](#toc0_)\n",
    "\n",
    "This experiment aimed to compare the effectiveness of two distinct image sharpening techniques—Unsharp Mask and Laplacian of Gaussian—when applied to a denoised and contrast-enhanced image from Experiment 2. The primary metrics for evaluation were PSNR and SSIM, supplemented by qualitative histogram analysis.\n",
    "\n",
    "##### 4.1.4.5.1. <a id='toc4_1_4_5_1_'></a>[Unsharp Mask](#toc0_)\n",
    "Unsharp Mask was selected for its straightforward implementation and effectiveness in enhancing edges. The method was configured through trial and error to achieve the most visually appealing result. The PSNR value stands at approximately 33.79, and the SSIM value is 0.946. Even though the method introduced some light noise, it significantly improved the texture of the image, making it more visually appealing. The histogram analysis indicates that Unsharp Mask doesn't introduce significant peaks in pixel intensity, except in the higher range (230+).\n",
    "\n",
    "##### 4.1.4.5.2. <a id='toc4_1_4_5_2_'></a>[Laplacian of Gaussian](#toc0_)\n",
    "Despite its theoretical promise, Laplacian of Gaussian was less effective visually compared to Unsharp Mask. It achieved a PSNR of about 51.18 and an SSIM value of 0.997. The histogram analysis for Laplacian of Gaussian showed multiple peaks at certain intensities, suggesting that the method might be introducing artifacts.\n",
    "\n",
    "##### 4.1.4.5.3. <a id='toc4_1_4_5_3_'></a>[PSNR and SSIM Metrics](#toc0_)\n",
    "The PSNR and SSIM values were consistent with the visual observations. Unsharp Mask, despite its lower PSNR and SSIM compared to Laplacian of Gaussian, was visually more appealing and effective in bringing out the texture of the image.\n",
    "\n",
    "##### 4.1.4.5.4. <a id='toc4_1_4_5_4_'></a>[Histogram Analysis](#toc0_)\n",
    "The histogram for Laplacian of Gaussian showed significant peaks at specific intensities, suggesting the introduction of artifacts. On the other hand, the histogram for Unsharp Mask was smoother, indicating a more natural enhancement.\n",
    "\n",
    "##### 4.1.4.5.5. <a id='toc4_1_4_5_5_'></a>[Performance](#toc0_)\n",
    "While performance metrics were not collected, the computational time for both methods was deemed not significantly impactful for the scope of this experiment.\n",
    "\n",
    "In summary, Unsharp Mask outperformed Laplacian of Gaussian in terms of visual appeal and texture enhancement. Although Laplacian of Gaussian showed higher PSNR and SSIM values, its visual output was not as effective. The experiment highlights the importance of using multiple metrics and qualitative assessments for a comprehensive evaluation.\n",
    "\n",
    "\n",
    "### 4.1.5. <a id='toc4_1_5_'></a>[Conclusion](#toc0_)\n",
    "This marks the end of the first task. \n",
    "\n",
    "The goal of this task was to remove air pollution from an image of Namsan Tower, Seoul, South Korea. We started by applying various contrast enhancement techniques to improve the visibility of the tower. We then applied denoising techniques to reduce the noise introduced by the contrast enhancement. Finally, we applied sharpening techniques to improve the clarity and detail of the image.\n",
    "\n",
    "While the end result isn't perfect, it is definitely a big improvement in comparison to the original image. The haze has been significantly reduced, and the tower is more discernible. The image is also more visually appealing, with improved texture and detail. The buildings are more visible, and the sky is clearer.\n",
    "\n",
    "Unfortunately the color of the image was lost in the process, but this was deemed acceptable given the primary objective of haze removal. The image could be further improved by applying color correction techniques, but this is beyond the scope of this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# original image vs sharpened image unsharp mask\n",
    "plot_before_after(image, processed_images_sharpened['Non-Local Means Denoised Image']['Unsharp Mask'], title1='Original', title2='Filtered Image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. <a id='toc4_2_'></a>[Signal properties](#toc0_)\n",
    "Search for different signals of different scenes suitable for your country or record some yourself. The signals should be suitable to demonstrate adjustments of the signal properties {'frequency, bandwidth, waveform'} in experiments. First measure the signal properties assigned to you on your signals using suitable metrics. Then define a target how the signal properties assigned to you should be changed. Define some experiments to achieve this goal and perform the experiments with your signals and suitable methods. Demonstrate the Nyquist Theorem with your recorded signals. Discuss your data, parameter, and method choices and the results obtained. \n",
    "\n",
    "\n",
    "**DISCLAIMER**: Even though I didn't have any problems I advice you to adjust the volume to prevent any damage to your ears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. <a id='toc4_2_1_'></a>[Data](#toc0_)\n",
    "\n",
    "The data for these experiments consists of two audio segments from popular K-Pop songs:\n",
    "1. \"What Type of X\" by Jessi (audio/Jessi_X_Audio_segment.wav): https://www.youtube.com/watch?v=R8d2LQXDKBg\n",
    "2. \"Everglow Dun Dun\" (audio/Everglow_Dun_Dun_segment.wav): https://www.youtube.com/watch?v=NoYKBAajoyo\n",
    "\n",
    "And also from a traditional Samulnori performance :\n",
    "1. \"Samulnori\" (audio/Samulnori.wav): https://www.youtube.com/watch?v=C5QinxMF6Sg\n",
    "\n",
    "The audio segments are 9.5 seconds long and were clipped from the original songs. These segments were chosen for their distinct audio characteristics, which will be explored in the subsequent experiments. (only vocals, only music, both)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "jessi_x_audio, jessi_x_sr = librosa.load('audio/Jessi_What_Type_of_X_segment.wav', sr=None)\n",
    "everglow_dun_dun_audio, everglow_dun_dun_sr = librosa.load('audio/Everglow_Dun_Dun_segment.wav', sr=None)\n",
    "samulnori_audio, samulnori_sr = librosa.load('audio/Samulnori_segment.wav', sr=None)\n",
    "\n",
    "plot_audio_waveform(jessi_x_audio, jessi_x_sr, 'Jessi What Type of X Audio Segment')\n",
    "plot_audio_waveform(everglow_dun_dun_audio, everglow_dun_dun_sr, 'Everglow Dun Dun Audio Segment')\n",
    "plot_audio_waveform(samulnori_audio, samulnori_sr, 'Samulnori Audio Segment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. <a id='toc4_2_2_'></a>[Experiment 1: Karaoke Time! Extracting Vocals and Music for Karaoke Versions using REPET-SIM Method](#toc0_)\n",
    "\n",
    "#### 4.2.2.1. <a id='toc4_2_2_1_'></a>[Problem](#toc0_)\n",
    "The popularity of karaoke in South Korean culture is undeniable, yet the availability of separate vocal and instrumental tracks for popular songs can be limited. This poses a challenge for karaoke enthusiasts who seek a more authentic and immersive singing experience. Our aim is to address this gap by extracting the vocal and instrumental components from popular tracks, such as \"Jessi - What Type of X\" and \"Everglow - Dun Dun.\" By doing so, we hope to enhance the karaoke experience, enabling singers to perform with either the original vocals or just the instrumental backing.\n",
    "\n",
    "For simplicity (and legal reasons) we will only look at a 10-second segment of each song. \n",
    "The whole idea for this came from this link: [Librosa Gallery Example](https://librosa.org/librosa_gallery/auto_examples/plot_vocal_separation.html)\n",
    "\n",
    "#### 4.2.2.2. <a id='toc4_2_2_2_'></a>[Objective](#toc0_)\n",
    "The primary goal of this experiment is to separate vocals from the musical components of selected K-Pop audio tracks. The aim is to create karaoke versions of the songs where the vocals and music can be isolated and played back independently. This experiment is particularly significant in the context of South Korean culture, where karaoke is an important social and entertainment activity.\n",
    "\n",
    "#### 4.2.2.3. <a id='toc4_2_2_3_'></a>[Approach](#toc0_)\n",
    "To achieve this, we will focus on two popular K-Pop songs:\n",
    "1. \"What Type of X\" by Jessi (audio/Jessi_What_Type_of_X_segment.wav)\n",
    "2. \"Everglow Dun Dun\" (audio/Everglow_Dun_Dun_segment.wav)\n",
    "\n",
    "We will utilize the REPET-SIM method, which is a sophisticated audio signal processing technique for vocal separation. The method works by transforming the audio signal into the frequency domain and applying a soft mask to isolate vocals from the musical components. We'll employ the following techniques:\n",
    "1. Short-Time Fourier Transform (STFT)\n",
    "2. Non-local filtering to create a soft mask\n",
    "3. Soft-masking to isolate vocals and music\n",
    "\n",
    "#### 4.2.2.4. <a id='toc4_2_2_4_'></a>[Metrics and Tools](#toc0_)\n",
    "\n",
    "##### 4.2.2.4.1. <a id='toc4_2_2_4_1_'></a>[Signal-to-Noise Ratio (SNR)](#toc0_)\n",
    "This metric quantifies the quality of the vocal separation by comparing the power of the separated musical background to the noise introduced by the separation process. It is expressed in decibels (dB), and higher values indicate better separation quality.\n",
    "\n",
    "##### 4.2.2.4.2. <a id='toc4_2_2_4_2_'></a>[Spectral Contrast](#toc0_)\n",
    "Spectral contrast provides insights into how distinct the vocal and musical components are in the frequency domain after separation. It measures the difference in amplitude between peaks and valleys in the spectrum for each frequency band. The average spectral contrast across all bands is used for evaluation.\n",
    "\n",
    "##### 4.2.2.4.3. <a id='toc4_2_2_4_3_'></a>[Audio Playback](#toc0_)\n",
    "An embedded audio player will be used for qualitative assessment. It allows for listening to the separated tracks to evaluate the effectiveness of the vocal and music isolation. This offers a subjective measure of the quality of separation.\n",
    "\n",
    "#### 4.2.2.5. <a id='toc4_2_2_5_'></a>[Methods](#toc0_)\n",
    "\n",
    "1. **STFT (Short-Time Fourier Transform)**: This method converts the time-domain audio signals into the frequency domain. It helps identify the frequencies where vocals and musical instruments are prominent.\n",
    "\n",
    "2. **Non-local Filtering and Soft Mask**: Based on the STFT, we apply non-local filtering to create a soft mask for isolating vocals from the musical background. This is accomplished using the REPET-SIM method. The quality of this separation is evaluated using the SNR and Spectral Contrast metrics.\n",
    "\n",
    "By incorporating these additional metric descriptions, the experiment documentation becomes more comprehensive and informs the reader more effectively about the evaluation methods employed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict, title_prefix):\n",
    "    \"\"\"Display metrics in a formatted manner.\"\"\"\n",
    "    print(f'--- Metrics for {title_prefix} ---')\n",
    "    for key, value in metrics_dict.items():\n",
    "        print(f'{key}: {value}')\n",
    "    print('-------------------------------')\n",
    "\n",
    "def calculate_snr(original, processed):\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio (SNR)\"\"\"\n",
    "    min_length = min(len(original), len(processed))\n",
    "    original = original[:min_length]\n",
    "    processed = processed[:min_length]\n",
    "    signal = np.abs(original)\n",
    "    noise = np.abs(original - processed)\n",
    "    return 10 * np.log10(np.mean(signal ** 2) / np.mean(noise ** 2))\n",
    "\n",
    "def calculate_spectral_contrast(S, sr):\n",
    "    \"\"\"Calculate spectral contrast\"\"\"\n",
    "    contrast = librosa.feature.spectral_contrast(S=S, sr=sr)\n",
    "    return np.mean(contrast, axis=1)\n",
    "\n",
    "\n",
    "def plot_waveform_and_spectrogram(audio, sr, title, normalize=True):\n",
    "    \"\"\"\n",
    "    Display the audio waveform and spectrogram in separate plots using matplotlib and include a play button.\n",
    "\n",
    "    Args:\n",
    "        audio (numpy.ndarray): The audio signal.\n",
    "        sr (int): The sampling rate of the audio signal.\n",
    "        title (str): The title for the plots.\n",
    "        normalize (bool): If True, normalizes the audio for the player.\n",
    "    \"\"\"\n",
    "    # Display audio player\n",
    "    display(Audio(audio, rate=sr, normalize=normalize))\n",
    "\n",
    "    # Create a GridSpec layout\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    gs = GridSpec(2, 2, width_ratios=[30, 1], height_ratios=[1, 2], wspace=0.2, hspace=0.3)\n",
    "\n",
    "    # Waveform plot\n",
    "    ax_waveform = fig.add_subplot(gs[0, 0])\n",
    "    time_axis = np.linspace(0, len(audio) / sr, num=len(audio))\n",
    "    ax_waveform.plot(time_axis, audio)\n",
    "    ax_waveform.set_xlim(left=0, right=time_axis[-1])  # Set x-axis limits to match the audio length\n",
    "    ax_waveform.set_title(f'{title} - Waveform')\n",
    "    ax_waveform.set_xlabel('Time [s]')\n",
    "    ax_waveform.set_ylabel('Amplitude')\n",
    "\n",
    "    # Spectrogram plot\n",
    "    ax_spectrogram = fig.add_subplot(gs[1, 0])\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax_spectrogram)\n",
    "    ax_spectrogram.set_title(f'{title} - Spectrogram')\n",
    "    ax_spectrogram.set_xlabel('Time [s]')\n",
    "    ax_spectrogram.set_ylabel('Frequency [Hz]')\n",
    "\n",
    "    # Colorbar\n",
    "    ax_colorbar = fig.add_subplot(gs[1, 1])\n",
    "    fig.colorbar(img, cax=ax_colorbar, format='%+2.0f dB')\n",
    "\n",
    "    # plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def separate_vocals_and_music(audio, sr, margin_i=2, margin_v=10, power=2):\n",
    "    \"\"\"Separate vocals and music from an audio signal\"\"\"\n",
    "    S_full, phase = librosa.magphase(librosa.stft(audio))\n",
    "    S_filter = librosa.decompose.nn_filter(S_full,\n",
    "                                           aggregate=np.median,\n",
    "                                           metric='cosine',\n",
    "                                           width=int(librosa.time_to_frames(2, sr=sr)))\n",
    "    S_filter = np.minimum(S_full, S_filter)\n",
    "    mask_i = librosa.util.softmask(S_filter, margin_i * (S_full - S_filter), power=power)\n",
    "    mask_v = librosa.util.softmask(S_full - S_filter, margin_v * S_filter, power=power)\n",
    "    S_background = mask_i * S_full\n",
    "    S_foreground = mask_v * S_full\n",
    "    background_audio = librosa.istft(S_background * phase)\n",
    "    foreground_audio = librosa.istft(S_foreground * phase)\n",
    "    return background_audio, foreground_audio, S_full, S_background, S_foreground\n",
    "\n",
    "def calculate_and_show_metrics(title_prefix, sr, spectrogram_dict, snr=None):\n",
    "    \"\"\"Display all metrics, spectrograms, and waveforms\"\"\"\n",
    "    contrast = calculate_spectral_contrast(spectrogram_dict['Full'], sr)\n",
    "    metrics = {'Spectral Contrast': contrast}\n",
    "    if snr:\n",
    "        metrics['SNR (dB)'] = snr\n",
    "    plot_metrics(metrics, title_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fine-tuning parameters based on trying around \n",
    "jessi_margin_i, jessi_margin_v = 15, 9 \n",
    "everglow_margin_i, everglow_margin_v = 10, 4\n",
    "\n",
    "# Apply the REPET-SIM method\n",
    "jessi_x_music, jessi_x_vocals, S_full_jessi, S_background_jessi, S_foreground_jessi = separate_vocals_and_music(\n",
    "    jessi_x_audio, jessi_x_sr, margin_i=jessi_margin_i, margin_v=jessi_margin_v)\n",
    "\n",
    "everglow_dun_dun_music, everglow_dun_dun_vocals, S_full_everglow, S_background_everglow, S_foreground_everglow = separate_vocals_and_music(\n",
    "    everglow_dun_dun_audio, everglow_dun_dun_sr, margin_i=everglow_margin_i, margin_v=everglow_margin_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.2.2.6. <a id='toc4_2_2_6_'></a>[What Type of X by Jessi](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_waveform_and_spectrogram(jessi_x_audio, jessi_x_sr, title='Jessi What Type of X Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The plot above provides a visual representation of the audio signal in the time and frequency domains. The brighter colors indicate higher amplitudes, which correspond to the vocals and music in the audio signal. You can also notice the visible change at 6 seconds which is the build up for the drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_waveform_and_spectrogram(jessi_x_music, jessi_x_sr, title='Jessi What Type of X Music')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we are looking at the music after the separation. The vocals are gone, however, the result is less than ideal. The music is distorted and sounds like a hardstep or hardcore track. The method was unable to separate the vocals and music cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_waveform_and_spectrogram(jessi_x_vocals, jessi_x_sr, title='Jessi What Type of X Vocals')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the vocals only, we can see that the vocals are not completely isolated. There are too many frequencies in the background. The vocals are however not distorted, but a bit shallow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spectrogram_dict_jessi = {\n",
    "    'Full': S_full_jessi,\n",
    "    'Background': S_background_jessi,\n",
    "    'Foreground': S_foreground_jessi\n",
    "}\n",
    "\n",
    "snr_jessi = calculate_snr(jessi_x_audio, jessi_x_music)\n",
    "\n",
    "calculate_and_show_metrics('Jessi What Type of X', jessi_x_sr, spectrogram_dict_jessi, snr=snr_jessi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.7. <a id='toc4_2_2_7_'></a>[Dun Dun by Everglow](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_waveform_and_spectrogram(everglow_dun_dun_audio, everglow_dun_dun_sr, title='Everglow Dun Dun Original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the song Dun Dun we can clearly see the two halfs which make up the segment: The first half which contains the vocals with little sound, followed by a drop at around 5 seconds with a lot of sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_waveform_and_spectrogram(everglow_dun_dun_music, everglow_dun_dun_sr, title='Everglow Dun Dun Music')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we are looking at the music only after the separation. The vocals are gone, however, this is not a karaoke version yet. The music is distorted and sounds like a hardstep or hardcore track. The method was unable to separate the vocals and music cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_waveform_and_spectrogram(everglow_dun_dun_vocals, everglow_dun_dun_sr, title='Everglow Dun Dun Vocals')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the vocals only, we can see that the vocals are not completely isolated. There are too many frequencies in the background. The vocals are however not distorted, but a bit shallow (echo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "spectrogram_dict_everglow = {\n",
    "    'Full': S_full_everglow,\n",
    "    'Background': S_background_everglow,\n",
    "    'Foreground': S_foreground_everglow\n",
    "}\n",
    "\n",
    "snr_everglow = calculate_snr(everglow_dun_dun_audio, everglow_dun_dun_music)\n",
    "\n",
    "calculate_and_show_metrics('Everglow Dun Dun', everglow_dun_dun_sr, spectrogram_dict_everglow, snr=snr_everglow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.8. <a id='toc4_2_2_8_'></a>[Results and Analysis](#toc0_)\n",
    "\n",
    "##### 4.2.2.8.1. <a id='toc4_2_2_8_1_'></a>[Metrics Analysis](#toc0_)\n",
    "\n",
    "1. **Spectral Contrast**: For \"Jessi What Type of X\", the spectral contrast values ranged from 15 to 47.5, indicating a fair amount of distinction between vocals and music in various frequency bands. In contrast, \"Everglow Dun Dun\" had values mostly in the 14 to 18 range, showing a more balanced but less distinct separation. \n",
    "\n",
    "2. **SNR (dB)**: The SNR values for both tracks were relatively low, with \"Jessi What Type of X\" having an SNR of 1.24 dB and \"Everglow Dun Dun\" at 0.89 dB. This suggests that the noise introduced during separation was significant, affecting the quality of the resulting tracks.\n",
    "\n",
    "##### 4.2.2.8.2. <a id='toc4_2_2_8_2_'></a>[Spectrogram Plots](#toc0_)\n",
    "\n",
    "The spectrogram plots give us a visual representation of how frequencies are distributed over time in the audio tracks. Areas of higher amplitude (brighter colors) in the spectrogram often correspond to either vocals or prominent instruments. A more distinct separation in the plots between vocals and music would indicate effective isolation, which was not entirely the case here.\n",
    "\n",
    "##### 4.2.2.8.3. <a id='toc4_2_2_8_3_'></a>[Effectiveness of Vocal and Music Separation](#toc0_)\n",
    "\n",
    "1. **Vocal Extraction**: The vocal extraction better than expected, especially considering the complex nature of the tracks. It could serve as vocal support for cover songs. In the second half there are still some fragments of the drop.\n",
    "\n",
    "2. **Music Distortion**: The separated music tracks were heavily distorted, making them hardly recognizable as K-Pop. They resembled genres like hardstep or hardcore more closely, suggesting a different use-case altogether.\n",
    "\n",
    "##### 4.2.2.8.4. <a id='toc4_2_2_8_4_'></a>[Challenges and Limitations](#toc0_)\n",
    "\n",
    "1. **Nature of the Songs**: The song \"Dun Dun\" starts with vocals and introduces music later, making the separation more challenging.\n",
    "\n",
    "2. **Auto-tuning**: The use of auto-tuning, especially in \"Dun Dun\", likely complicated the separation process, as it alters the natural frequency characteristics of the vocals.\n",
    "\n",
    "3. **Parameter Tuning**: While parameter tuning improved vocal extraction, it did not significantly improve the quality of the separated music.\n",
    "\n",
    "##### 4.2.2.8.5. <a id='toc4_2_2_8_5_'></a>[Summary and Future Directions](#toc0_)\n",
    "\n",
    "In summary, the experiment was somewhat successful in isolating vocals but not in creating usable karaoke tracks. The low SNR values and high spectral contrast indicate room for improvement. Challenges like the nature of the songs and auto-tuning further complicated the task. With more time and resources, the experiment could be extended to explore other methods for vocal separation, such as deep learning-based approaches. However, this would be out of scope for the nature of this mini challenge. \n",
    "\n",
    "On the other side, the results would definitely be something one could hear at a rave, which itself may be something else to look at.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. <a id='toc4_2_3_'></a>[Experiment 2: Frequency Analysis of Samulnori Music Using Bandpass Filtering](#toc0_)\n",
    "\n",
    "#### 4.2.3.1. <a id='toc4_2_3_1_'></a>[Problem](#toc0_)\n",
    "Despite its rich heritage, traditional Korean Samulnori music has been relatively unexplored in the realm of digital signal processing. By decomposing the audio signal into distinct frequency bands, we can uncover the unique acoustical traits inherent to this genre.\n",
    "\n",
    "#### 4.2.3.2. <a id='toc4_2_3_2_'></a>[Objective](#toc0_)\n",
    "To discern different frequency spectrums tied to specific instruments within a Samulnori track. The methodology involves partitioning the audio into three primary frequency bands: low, mid, and high, while zeroing in on the frequencies linked to Samulnori's four main instruments.\n",
    "\n",
    "#### 4.2.3.3. <a id='toc4_2_3_3_'></a>[Approach](#toc0_)\n",
    "For this experiment, the focus is on a 9.5-second segment of a Samulnori performance (audio/Samulnori_segment.wav). The segment has distinct audio characteristics, making it ideal for deep analysis.\n",
    "\n",
    "1. Examine the frequency spectrum of the original audio signal using advanced techniques suitable for non-stationary signals.\n",
    "2. Apply bandpass filters dynamically, based on preliminary frequency analysis.\n",
    "3. Analyze the frequency spectrum of the filtered signals.\n",
    "4. Calculate the Signal-to-Noise Ratio (SNR) and Frequency Matching Score to evaluate the effectiveness of the bandpass filters.\n",
    "5. Visualize the original and filtered audio waveforms for a more tangible understanding of the impact.\n",
    "\n",
    "#### 4.2.3.4. <a id='toc4_2_3_4_'></a>[Metrics and Tools](#toc0_)\n",
    "##### 4.2.3.4.1. <a id='toc4_2_3_4_1_'></a>[Signal-to-Noise Ratio (SNR)](#toc0_)\n",
    "This metric will be used to quantify how well the bandpass filters isolate frequencies associated with specific instruments from the background noise.\n",
    "\n",
    "##### 4.2.3.4.2. <a id='toc4_2_3_4_2_'></a>[Frequency Matching Score](#toc0_)\n",
    "This score will be based on the overlap between the energy distribution of the filtered signal and the characteristic frequencies of the specific instruments in Samulnori music.\n",
    "\n",
    "#### 4.2.3.5. <a id='toc4_2_3_5_'></a>[Methods](#toc0_)\n",
    "1. **Fast Fourier Transform (FFT)**: The FFT is employed as a core technique to manage the complexity of non-stationary signals such as music. By transforming the time-domain signal into the frequency domain, the FFT unveils the intricate frequency components with efficiency and precision.\n",
    "2. **Dynamic Bandwidth Estimation**: Leveraging the insights gained from the Short-Time Fourier Transform (STFT), this method dynamically discerns the optimal frequency ranges. These ranges are pivotal for crafting bandpass filters that are tailored to the signal's unique characteristics.\n",
    "3. **Dynamic Bandpass Filtering**: With the dynamically estimated frequency ranges in hand, this approach applies bandpass filters that are finely tuned to isolate the frequencies of interest. This selective filtering accentuates the desired signal attributes while mitigating extraneous noise.\n",
    "4. **Signal-to-Noise Ratio (SNR)**: The SNR calculation stands as a quantitative benchmark to assess the purity of the filtered frequencies. A higher SNR indicates a more effective filtration, signifying a clearer distinction between the signal and background noise.\n",
    "5. **Frequency Matching Score**: This metric delves into the filtered signal's energy distribution, juxtaposing it against the signature frequencies of the Samulnori instruments. The objective is to gauge how closely the filtered signal mirrors the authentic sound profiles.\n",
    "6. **Audio Waveform Visualization**: The visual representation of audio waveforms, both before and after filtering, serves as an illustrative tool. It provides an immediate, visual comprehension of the filter's impact, enhancing our understanding of the changes wrought upon the original audio signal.\n",
    "\n",
    "Through the lens of bandpass filtering to segregate distinct frequency bands, this experiment seeks to enrich our comprehension of the distinctive frequency attributes woven into Samulnori music.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bandpass_filter(signal, sr, lower_freq, upper_freq):\n",
    "    N = len(signal)\n",
    "    freqs = np.fft.fftfreq(N, 1/sr)\n",
    "    signal_fft = fft(signal)\n",
    "\n",
    "    for i, freq in enumerate(freqs):\n",
    "        if abs(freq) < lower_freq or abs(freq) > upper_freq:\n",
    "            signal_fft[i] = 0\n",
    "\n",
    "    return np.real(ifft(signal_fft))\n",
    "\n",
    "def frequency_matching_score(filtered_spectrum, reference_spectrum):\n",
    "    if len(filtered_spectrum) != len(reference_spectrum):\n",
    "        raise ValueError('Both spectrums should have the same length.')\n",
    "\n",
    "    # Calculate the cosine similarity between the two frequency spectrums\n",
    "    score = 1 - distance.cosine(filtered_spectrum, reference_spectrum)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def plot_frequency_spectrum(signal, sr, title):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.magnitude_spectrum(signal, Fs=sr)\n",
    "    plt.title(f'Frequency Spectrum of {title}')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_psd(signal, sr, title):\n",
    "    f, Pxx = welch(signal, sr)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.semilogy(f, Pxx)\n",
    "    plt.title(f'Power Spectral Density of {title}')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('PSD [V**2/Hz]')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Frequency Spectrum for original and filtered signals\n",
    "plot_frequency_spectrum(samulnori_audio, samulnori_sr, 'Original')\n",
    "\n",
    "# Plot PSD for original and filtered signals\n",
    "plot_psd(samulnori_audio, samulnori_sr, 'Original')\n",
    "\n",
    "plot_waveform_and_spectrogram(samulnori_audio, samulnori_sr, title='Samulnori Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the plots it becomes clear how all frequencies are present throughout the whole segment. There appear to be some stronger frequencies at 128Hz and 500Hz. The PSD plot supports this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define frequency ranges for low, mid, and high\n",
    "# ranges are based on patterns identified in the frequency spectrum and common practices for bandpass filtering\n",
    "low_range = (0, 500)\n",
    "mid_range = (500, 2000)\n",
    "high_range = (2000, 8000)\n",
    "\n",
    "# Apply bandpass filters\n",
    "low_pass_audio = bandpass_filter(samulnori_audio, samulnori_sr, *low_range)\n",
    "mid_pass_audio = bandpass_filter(samulnori_audio, samulnori_sr, *mid_range)\n",
    "high_pass_audio = bandpass_filter(samulnori_audio, samulnori_sr, *high_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_frequency_spectrum(low_pass_audio, samulnori_sr, 'Low-Pass Filtered')\n",
    "plot_psd(low_pass_audio, samulnori_sr, 'Low-Pass Filtered')\n",
    "\n",
    "fms_low = frequency_matching_score(low_pass_audio, samulnori_audio)\n",
    "snr_low = calculate_snr(samulnori_audio, low_pass_audio)\n",
    "\n",
    "plot_waveform_and_spectrogram(low_pass_audio, samulnori_sr, title='Low-Pass Filtered Samulnori Audio')\n",
    "plot_metrics({'Frequency Matching Score': fms_low, 'SNR': snr_low}, 'Low-Pass Filtered Samulnori Audio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fms_mid = frequency_matching_score(mid_pass_audio, samulnori_audio)\n",
    "snr_mid = calculate_snr(samulnori_audio, mid_pass_audio)\n",
    "\n",
    "plot_frequency_spectrum(mid_pass_audio, samulnori_sr, 'Mid-Pass Filtered')\n",
    "plot_psd(mid_pass_audio, samulnori_sr, 'Mid-Pass Filtered')\n",
    "\n",
    "\n",
    "plot_waveform_and_spectrogram(mid_pass_audio, samulnori_sr, title='Mid-Pass Filtered Samulnori Audio')\n",
    "plot_metrics({'Frequency Matching Score': fms_mid, 'SNR': snr_mid}, 'Mid-Pass Filtered Samulnori Audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fms_high = frequency_matching_score(high_pass_audio, samulnori_audio)\n",
    "snr_high = calculate_snr(samulnori_audio, high_pass_audio)\n",
    "\n",
    "plot_frequency_spectrum(high_pass_audio, samulnori_sr, 'High-Pass Filtered')\n",
    "plot_psd(high_pass_audio, samulnori_sr, 'High-Pass Filtered')\n",
    "\n",
    "plot_waveform_and_spectrogram(high_pass_audio, samulnori_sr, title='High-Pass Filtered Samulnori Audio')\n",
    "plot_metrics({'Frequency Matching Score': fms_high, 'SNR': snr_high}, 'High-Pass Filtered Samulnori Audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3.6. <a id='toc4_2_3_6_'></a>[Results and Analysis](#toc0_)\n",
    "\n",
    "The second experiment embarked on a deep dive into traditional Korean Samulnori music's frequency spectrums using advanced bandpass filtering.\n",
    "\n",
    "**Fourier Transform and Bandwidth Estimation**  \n",
    "Upon applying the Fourier Transform to the Samulnori track, clear peaks became evident around:\n",
    "- ~300 Hz: This peak, boasting the highest magnitude, possibly represents a fundamental frequency or a dominant harmonic intrinsic to Samulnori.\n",
    "- ~500 Hz: Another pronounced peak which might allude to another prevailing harmonic or a characteristic Samulnori frequency.\n",
    "Further, minor peaks were discernible in the range from 0 to 1000 Hz, suggesting the presence of various harmonics or instruments. Smaller, but still noteworthy, peaks were spotted around ~1500 Hz and ~2900 Hz. Frequencies post the 3000 Hz mark exhibited diminished magnitudes, signifying the absence of dominant frequencies in the elevated range.\n",
    "\n",
    "**Power Spectral Density (PSD) Analysis**  \n",
    "The PSD graph showcased a dominant hump between 0 and 1000 Hz, corroborating the observed peaks in the FFT-based frequency spectrum. The power steadily decreased post the 1000 Hz range, suggesting less energy in the higher frequencies.\n",
    "\n",
    "**Bandpass Filters and Their Efficacy**  \n",
    "The efficacy of the bandpass filters was evaluated using the newly-introduced Frequency Matching Score and SNR:\n",
    "\n",
    "- **Low-Pass Filtered Audio**:  \n",
    "  - Frequency Matching Score: 0.925  \n",
    "  - SNR: 8.429\n",
    "  \n",
    "- **Mid-Pass Filtered Audio**:  \n",
    "  - Frequency Matching Score: 0.299  \n",
    "  - SNR: 0.408\n",
    "  \n",
    "- **High-Pass Filtered Audio**:  \n",
    "  - Frequency Matching Score: 0.222  \n",
    "  - SNR: 0.220\n",
    "\n",
    "The low-pass filter yielded the highest scores, suggesting that the essence of Samulnori largely resides in lower frequencies. However, the scores for the mid and high-pass filters were considerably lower, raising questions about potential biases in focusing on the most 'energetic' frequencies.\n",
    "\n",
    "**Limitations and Constraints**  \n",
    "While our proprietary metrics offer insights, they are not universally recognized and may have limitations in interpretation. For example, a high SNR in the low-pass filter could suggest either effective isolation or simply a strong resemblance to the original signal.\n",
    "\n",
    "**Future Directions**  \n",
    "Future work could involve refining the bandpass filters or employing machine learning techniques to automatically identify instrument-specific frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. <a id='toc4_2_4_'></a>[Experiment 3: Waveform Analysis and Transformation of \"Dun Dun\" Remix](#toc0_)\n",
    "\n",
    "#### 4.2.4.1. <a id='toc4_2_4_1_'></a>[Problem](#toc0_)\n",
    "Understanding the waveform of an audio signal is crucial for insights into its temporal characteristics. Transforming this waveform offers an understanding of how various modifications impact the auditory perception and the harmonic-to-noise ratio of the audio segment.\n",
    "\n",
    "With the results of experiment 1 it became clear that removing the vocals from the song was not easy with basic tools. So now we will go into the other direction and try to add some effects to the song. A remix. \n",
    "\n",
    "#### 4.2.4.2. <a id='toc4_2_4_2_'></a>[Objective](#toc0_)\n",
    "To visually inspect and compare the waveform of a specific segment from the \"Dun Dun\" track before and after applying amplitude modulations. The goal is to analyze the effects of waveform transformations on the segment's shape, amplitude variations, and harmonic-to-noise ratio.\n",
    "\n",
    "#### 4.2.4.3. <a id='toc4_2_4_3_'></a>[Approach](#toc0_)\n",
    "1. Extract a specific segment from the \"Dun Dun\" track.\n",
    "2. Implement amplitude modulations to the segment, thereby creating a transformed or \"remixed\" version.\n",
    "3. Plot and contrast the waveforms of both the original and the remixed segments.\n",
    "4. Calculate and compare the Harmonic-to-Noise Ratio (HNR) for the two segments.\n",
    "\n",
    "#### 4.2.4.4. <a id='toc4_2_4_4_'></a>[Metrics and Tools](#toc0_)\n",
    "##### 4.2.4.4.1. <a id='toc4_2_4_4_1_'></a>[Waveform Visualization](#toc0_)\n",
    "A side-by-side plot displaying the variations in amplitude of the audio signal for both the original and remixed segments. This serves as a subjective measure to understand the impact of waveform transformations on the audio.\n",
    "\n",
    "##### 4.2.4.4.2. <a id='toc4_2_4_4_2_'></a>[Harmonic-to-Noise Ratio (HNR)](#toc0_)\n",
    "A quantitative metric that delves into the ratio between the harmonic (periodic) content and noise (aperiodic) content in an audio signal. A greater HNR value denotes a clearer harmonic presence, which can be influenced by the applied waveform transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def harmonic_to_noise_ratio(y):\n",
    "    \"\"\"\n",
    "    Compute the Harmonic-to-Noise Ratio (HNR) of an audio signal.\n",
    "    \"\"\"\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    harmonic_energy = np.sum(y_harmonic**2)\n",
    "    noise_energy = np.sum(y_percussive**2)\n",
    "    hnr = 10 * np.log10(harmonic_energy / (noise_energy + 1e-6))\n",
    "    return hnr\n",
    "\n",
    "def apply_modulation(audio, start_freq, end_freq, duration):\n",
    "    \"\"\"\n",
    "    Apply amplitude modulation to the given audio segment.\n",
    "    \"\"\"\n",
    "    num_samples = len(audio)\n",
    "    time = np.linspace(0, duration, num_samples)\n",
    "    frequencies = np.linspace(start_freq, end_freq, num_samples)\n",
    "    modulation = np.sin(2 * np.pi * frequencies * time)\n",
    "    return audio * modulation\n",
    "\n",
    "sr, audio = everglow_dun_dun_sr, everglow_dun_dun_audio\n",
    "\n",
    "# Get duration of the full clip\n",
    "total_duration = len(audio) / sr\n",
    "\n",
    "# Segmenting the audio for build-up and drop\n",
    "build_up_duration = 4.8 \n",
    "drop_duration = total_duration - build_up_duration  \n",
    "\n",
    "build_up = audio[:int(build_up_duration * sr)]\n",
    "drop = audio[int(build_up_duration * sr):]\n",
    "\n",
    "# Applying modulation to the build-up with increasing frequency\n",
    "modulated_build_up = apply_modulation(build_up, 0, 300, build_up_duration)\n",
    "\n",
    "# Applying consistent modulation to the drop\n",
    "modulated_drop = apply_modulation(drop, 50, 50, drop_duration)\n",
    "\n",
    "# Combining the segments for the remixed track\n",
    "remixed_track = np.concatenate([modulated_build_up, modulated_drop])\n",
    "\n",
    "plot_waveform_and_spectrogram(audio, sr, 'Original Dun Dun Audio Segment')\n",
    "plot_waveform_and_spectrogram(remixed_track, sr, 'Remixed Dun Dun Audio Segment')\n",
    "\n",
    "original_hnr = harmonic_to_noise_ratio(audio)\n",
    "remixed_hnr = harmonic_to_noise_ratio(remixed_track)\n",
    "\n",
    "print(f'Original Dun Dun HNR: {original_hnr:.2f} dB')\n",
    "print(f'Remixed Dun Dun HNR: {remixed_hnr:.2f} dB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4.5. <a id='toc4_2_4_5_'></a>[Results and Analysis](#toc0_)\n",
    "\n",
    "##### 4.2.4.5.1. <a id='toc4_2_4_5_1_'></a>[Waveform Visualization](#toc0_)\n",
    "\n",
    "The waveform plots provide a visual representation of the differences between the original and remixed segments:\n",
    "\n",
    "- **Original Dun Dun Audio Segment**: Shows a consistent amplitude across the segment, punctuated by the track's inherent rhythmic patterns and melodic content.\n",
    "  \n",
    "- **Remixed Dun Dun Audio Segment**: Displays pronounced amplitude modulations. In the build-up, these modulations increase in frequency from 0 to 300 Hz, adding dynamic tension that is resolved in the drop.\n",
    "\n",
    "##### 4.2.4.5.2. <a id='toc4_2_4_5_2_'></a>[Spectrogram Analysis](#toc0_)\n",
    "\n",
    "The spectrogram of the remixed segment visually illustrates the gradual increase in dominant frequencies over time during the build-up, which is a testament to the strategic frequency modulation applied.\n",
    "\n",
    "##### 4.2.4.5.3. <a id='toc4_2_4_5_3_'></a>[Harmonic-to-Noise Ratio (HNR)](#toc0_)\n",
    "\n",
    "Comparing the HNR values of both segments:\n",
    "\n",
    "- Original Segment HNR: **8.24 dB**\n",
    "- Remixed Segment HNR: **7.53 dB**\n",
    "\n",
    "The remixed version exhibits a slightly lower HNR, indicating that the amplitude modulations introduced have potentially increased the noise component or reduced the harmonic presence, thus affecting the audio's clarity.\n",
    "\n",
    "##### 4.2.4.5.4. <a id='toc4_2_4_5_4_'></a>[Methodical Amplitude Modulation](#toc0_)\n",
    "\n",
    "The code reveals a strategic approach to amplitude modulation:\n",
    "\n",
    "- **Build-up Segment**: The modulation frequency begins at 0 Hz and increases to 300 Hz, progressively building energy and anticipation.\n",
    "  \n",
    "- **Drop Segment**: A consistent modulation frequency of 50 Hz maintains a steady energy level, contrasting with the dynamic build-up.\n",
    "\n",
    "This increasing modulation frequency during the build-up, visible in the spectrogram, creates a compelling energy shift in the remixed track that significantly alters the listener's experience.\n",
    "\n",
    "##### 4.2.4.5.5. <a id='toc4_2_4_5_5_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "The analysis underscores the transformative impact of amplitude modulation on the temporal dynamics and overall sound quality of an audio segment. The careful escalation of modulation frequency during the build-up phase, clearly observable in the spectrogram, contributes to a heightened auditory experience. However, this must be balanced with preserving the original track's harmonic quality. Future studies could further investigate different modulation strategies and their effects on various musical genres, with an emphasis on understanding listener response through predictive analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. <a id='toc4_3_'></a>[Experiment 4: Demonstrating the Nyquist Theorem using Samulnori Music](#toc0_)\n",
    "\n",
    "### 4.3.1. <a id='toc4_3_1_'></a>[Problem](#toc0_)\n",
    "To validate the Nyquist Theorem, which is fundamental in signal processing, especially when converting a continuous signal to a discrete one.\n",
    "\n",
    "### 4.3.2. <a id='toc4_3_2_'></a>[Objective](#toc0_)\n",
    "To demonstrate the importance of sampling rate by comparing the original audio segment with its reconstructed versions that have been sampled at different rates.\n",
    "\n",
    "### 4.3.3. <a id='toc4_3_3_'></a>[Approach](#toc0_)\n",
    "1. Identify the highest frequency present in a selected Samulnori segment using STFT for better accuracy.\n",
    "2. Apply an anti-aliasing filter before sampling the audio at rates below and above twice this highest frequency to prevent distortion.\n",
    "3. Sample the audio at these rates and attempt to reconstruct the original waveform from these samples.\n",
    "4. Compare the original and reconstructed waveforms visually and through waveform similarity scores.\n",
    "\n",
    "### 4.3.4. <a id='toc4_3_4_'></a>[Metrics and Tools](#toc0_)\n",
    "#### 4.3.4.1. <a id='toc4_3_4_1_'></a>[Waveform Similarity Score](#toc0_)\n",
    "This metric quantifies the similarity between the original and reconstructed waveforms. A high score denotes effective reconstruction, validating the Nyquist Theorem.\n",
    "\n",
    "### 4.3.5. <a id='toc4_3_5_'></a>[Methods](#toc0_)\n",
    "1. **Short-Time Fourier Transform (STFT)**: Used to accurately identify the highest frequency in the audio segment by analyzing the frequency content over short windows of time. This is crucial for determining the proper sampling rates for the experiment.\n",
    "2. **Anti-Aliasing Filter**: An anti-aliasing low-pass filter is applied before resampling to prevent the introduction of aliasing artifacts. This step ensures that the sampled audio does not contain frequency components above the Nyquist frequency, which could corrupt the waveform upon reconstruction.\n",
    "3. **Resampling**: Changing the sampling rate of the audio based on the identified highest frequency to create different versions of the waveform. This step is key to demonstrating the Nyquist Theorem's principle that sampling above a certain threshold is required for accurate reconstruction.\n",
    "4. **Waveform Reconstruction**: Using the resampled audio data to reconstruct the waveform, followed by visual comparison and similarity scoring to evaluate the effectiveness of the sampling and reconstruction process. It provides empirical evidence for the theoretical principles of the Nyquist Theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def highest_frequency(audio, sr):\n",
    "    \"\"\"\n",
    "    Identify the highest frequency in an audio segment using STFT for better accuracy.\n",
    "    \"\"\"\n",
    "    f, t, Zxx = signal.stft(audio, fs=sr, window='hann', nperseg=1024)\n",
    "    spectrum = np.abs(Zxx)\n",
    "    highest_freq = f[np.argmax(np.sum(spectrum, axis=1))]\n",
    "\n",
    "    return highest_freq\n",
    "\n",
    "def apply_anti_aliasing_filter(audio, cutoff, sr):\n",
    "    \"\"\"\n",
    "    Apply an anti-aliasing low-pass filter to the audio signal.\n",
    "    \"\"\"\n",
    "    sos = signal.butter(10, cutoff, 'lp', fs=sr, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    return filtered_audio\n",
    "\n",
    "def demonstrate_nyquist_theorem(audio, sr, highest_freq):\n",
    "    \"\"\"\n",
    "    Visualize the effects of the Nyquist theorem on a given audio segment.\n",
    "    \"\"\"\n",
    "    # Apply anti-aliasing filter before resampling\n",
    "    anti_aliased_audio = apply_anti_aliasing_filter(audio, highest_freq, sr)\n",
    "\n",
    "    sample_rates = [\n",
    "        (highest_freq * 2 - highest_freq * 0.5, 'Below Nyquist rate'),\n",
    "        (highest_freq * 2, 'Nyquist rate (lower bound)'),\n",
    "        (highest_freq * 4, 'Double Nyquist rate'),\n",
    "    ]\n",
    "\n",
    "    time = np.linspace(0, len(anti_aliased_audio)/sr, len(anti_aliased_audio))\n",
    "    n_plots = len(sample_rates) + 1\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    ax = fig.add_subplot(n_plots, 1, 1)\n",
    "    ax.plot(time, anti_aliased_audio, label=f'Original (Highest freq: {highest_freq:.2f}Hz)')\n",
    "    ax.legend(loc=3)\n",
    "    ax.set_title('Original Audio with Anti-Aliasing Filter')\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    for i, (sample_rate, title) in enumerate(sample_rates):\n",
    "        ax = fig.add_subplot(n_plots, 1, i+2)\n",
    "\n",
    "        resampled_time = np.linspace(0, len(anti_aliased_audio)/sr, int(len(anti_aliased_audio) * sample_rate/sr))\n",
    "        resampled_audio = np.interp(resampled_time, time, anti_aliased_audio)\n",
    "\n",
    "        ax.plot(time, anti_aliased_audio, label=f'Original Audio', color='0.8') # show original audio for reference\n",
    "        ax.plot(resampled_time, resampled_audio, marker='.', linestyle='None', label=f'Sampled at {sample_rate:.2f}Hz')\n",
    "        ax.set_ylim(*ylim)\n",
    "        ax.legend(loc=3)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming the audio is already loaded\n",
    "sr, audio_data = samulnori_sr, samulnori_audio\n",
    "\n",
    "# Select a short segment for clear visualization\n",
    "audio_data = audio_data[int(sr * 0.1):int(sr * 0.3)]\n",
    "\n",
    "# Identify the highest frequency using STFT\n",
    "highest_frequency_value = highest_frequency(audio_data, sr)\n",
    "\n",
    "# Demonstrate the Nyquist theorem\n",
    "demonstrate_nyquist_theorem(audio_data, sr, highest_frequency_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.6. <a id='toc4_3_6_'></a>[Reflection: Nyquist Theorem and Samulnori Music](#toc0_)\n",
    "\n",
    "#### 4.3.6.1. <a id='toc4_3_6_1_'></a>[Nyquist Theorem Validation](#toc0_)\n",
    "The experiment aimed to validate the Nyquist Theorem, asserting that a continuous signal can be completely represented and reconstructed if sampled at a rate at least twice its highest frequency.\n",
    "\n",
    "#### 4.3.6.2. <a id='toc4_3_6_2_'></a>[Observations from the Plot](#toc0_)\n",
    "1. **Below Nyquist Rate (193.80Hz)**: Consistent with expectations, sampling below the Nyquist rate compromised the waveform fidelity, resulting in a reconstruction that diverged from the original.\n",
    "\n",
    "2. **Nyquist Rate (258.40Hz)**: Theoretically, sampling at exactly twice the highest frequency should permit an exact reconstruction. However, the reconstructed waveform showed some slight deviations from the original, indicating that the Nyquist rate serves as a starting point rather than a guarantee for perfect reconstruction. Peaks seems to be missing in the reconstructed waveform.\n",
    "\n",
    "3. **Double Nyquist Rate (516.80Hz)**: Sampling at double the Nyquist rate yielded an excellent reconstruction of the original waveform, capturing the peaks and intricate details that were missed at lower sampling rates. This demonstrates that a higher sampling rate can indeed provide a more accurate representation of complex signals.\n",
    "\n",
    "#### 4.3.6.3. <a id='toc4_3_6_3_'></a>[Possible Reasons for Discrepancy](#toc0_)\n",
    "1. **Limitations of STFT**: While the STFT is adept at capturing the time-varying frequency content of signals, it might still have limitations in fully capturing the spectral complexity of rich, dynamic audio like Samulnori music.\n",
    "2. **Complexity of Samulnori Music**: The intricate layers and nuances of Samulnori music may necessitate a sampling rate beyond the basic Nyquist threshold to capture its full complexity.\n",
    "\n",
    "#### 4.3.6.4. <a id='toc4_3_6_4_'></a>[Conclusion](#toc0_)\n",
    "The Nyquist Theorem is a critical foundation in signal processing, but this experiment highlights its limitations in practical scenarios, particularly with complex audio signals like Samulnori music. While the theorem provides a theoretical minimum for sampling rates, real-world applications, especially those involving intricate and non-periodic signals, may benefit from sampling rates substantially higher than the Nyquist limit. This underscores the need for further exploration to refine our understanding and application of the theorem in various audio processing contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. <a id='toc4_4_'></a>[Normalization and Standardization](#toc0_)\n",
    "Normalize and standardize your image and signal data. Demonstrate that the data continues to show the same content after normalization/standardization. Discuss your method choice and the results obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_path = 'images/Namsan Tower Yonhap.jpeg'\n",
    "original_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "original_image_float = original_image.astype('float32')\n",
    "normalized_image = original_image_float / 255.0\n",
    "\n",
    "mean_val = np.mean(normalized_image)\n",
    "std_val = np.std(normalized_image)\n",
    "standardized_image = (normalized_image - mean_val) / std_val\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 12))\n",
    "\n",
    "# Original Image\n",
    "axes[0, 0].imshow(original_image)\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "axes[1, 0].hist(cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY).ravel(), 256, [0, 256], color='k')\n",
    "axes[1, 0].set_title('Original Histogram')\n",
    "axes[1, 0].set_xlabel('Pixel Intensity')\n",
    "axes[1, 0].set_ylabel('Pixel Count')\n",
    "\n",
    "# Normalized Image\n",
    "axes[0, 1].imshow(normalized_image)\n",
    "axes[0, 1].set_title('Normalized Image')\n",
    "axes[0, 1].axis('off')\n",
    "axes[1, 1].hist((normalized_image * 255).astype('uint8').ravel(), 256, [0, 256], color='k')\n",
    "axes[1, 1].set_title('Normalized Histogram')\n",
    "axes[1, 1].set_xlabel('Pixel Intensity')\n",
    "\n",
    "# Standardized Image\n",
    "axes[0, 2].imshow(standardized_image)\n",
    "axes[0, 2].set_title('Standardized Image')\n",
    "axes[0, 2].axis('off')\n",
    "axes[1, 2].hist((standardized_image * 255).astype('uint8').ravel(), 256, [0, 256], color='k')\n",
    "axes[1, 2].set_title('Standardized Histogram')\n",
    "axes[1, 2].set_xlabel('Pixel Intensity')\n",
    "\n",
    "visual_standardized_image = (standardized_image - standardized_image.min()) / (standardized_image.max() - standardized_image.min())\n",
    "# Visualized Standardized Image\n",
    "axes[0, 3].imshow(visual_standardized_image)\n",
    "axes[0, 3].set_title('Adjusted Standardized Image')\n",
    "axes[0, 3].axis('off')\n",
    "axes[1, 3].hist((visual_standardized_image * 255).astype('uint8').ravel(), 256, [0, 256], color='k')\n",
    "axes[1, 3].set_title('Standardized Histogram')\n",
    "axes[1, 3].set_xlabel('Pixel Intensity')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Original Image: Mean={np.mean(original_image):.2f}, Std={np.std(original_image):.2f}, Min={np.min(original_image)}, Max={np.max(original_image)}')\n",
    "print(f'Normalized Image: Mean={np.mean(normalized_image):.2f}, Std={np.std(normalized_image):.2f}, Min={np.min(normalized_image):.2f}, Max={np.max(normalized_image):.2f}')\n",
    "print(f'Standardized Image: Mean={np.mean(standardized_image):.2f}, Std={np.std(standardized_image):.2f}, Min={np.min(standardized_image):.2f}, Max={np.max(standardized_image):.2f}')\n",
    "print(f'Adjusted Standardized Image: Mean={np.mean(visual_standardized_image):.2f}, Std={np.std(visual_standardized_image):.2f}, Min={np.min(visual_standardized_image):.2f}, Max={np.max(visual_standardized_image):.2f}')\n",
    "\n",
    "# Step 6: Difference maps\n",
    "diff_normalized = np.abs(original_image_float - normalized_image * 255)\n",
    "diff_standardized = np.abs(original_image_float - standardized_image * 255)\n",
    "diff_visual_standardized = np.abs(original_image_float - visual_standardized_image * 255)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "axes[0].imshow(diff_normalized.astype('uint8'))\n",
    "axes[0].set_title('Difference Map: Original vs Normalized')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(diff_standardized.astype('uint8'))\n",
    "axes[1].set_title('Difference Map: Original vs Standardized')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(diff_visual_standardized.astype('uint8'))\n",
    "axes[2].set_title('Difference Map: Original vs Adjusted Standardized')\n",
    "axes[2].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. <a id='toc4_4_1_'></a>[Analysis: Image Normalization and Standardization](#toc0_)\n",
    "\n",
    "#### 4.4.1.1. <a id='toc4_4_1_1_'></a>[Normalization](#toc0_)\n",
    "Upon visual inspection, it's clear that normalizing the image has little to no noticeable impact on its visual perception. The content, colors, and features remain the same; only the intensity values are scaled between 0 and 1. Even though the histogram changes, representing the redistributed intensity levels, our visual interpretation of the image remains consistent with the original.\n",
    "\n",
    "#### 4.4.1.2. <a id='toc4_4_1_2_'></a>[Standardization](#toc0_)\n",
    "Standardization, on the other hand, significantly alters the appearance of the image. By centering the pixel values around the mean and scaling based on the standard deviation, standardization can lead to an image that is harder to interpret visually. The reason behind this drastic change is that standardization not only scales the pixel values but also shifts their distribution, making them conform to a standard normal distribution. As a result, features that were once distinguishable might become less apparent or even lost.\n",
    "\n",
    "##### 4.4.1.2.1. <a id='toc4_4_1_2_1_'></a>[Adjusted Standardization](#toc0_)\n",
    "The adjusted standardization is a simple linear transformation that scales the pixel values between 0 and 1. This transformation preserves the visual integrity of the image while still centering the pixel values around the mean and scaling them based on the standard deviation. This is why the adjusted standardization is often preferred over standardization in image processing.\n",
    "\n",
    "#### 4.4.1.3. <a id='toc4_4_1_3_'></a>[Difference Maps](#toc0_)\n",
    "The difference maps further highlight the impact of normalization and standardization. While the difference map for normalization is pitch black, indicating no change, the one for standardization is more varied, with some regions exhibiting a higher difference than others. This is because standardization not only scales the pixel values but also shifts their distribution, making them conform to a standard normal distribution. As a result, features that were once distinguishable might become less apparent or even lost.\n",
    "\n",
    "#### 4.4.1.4. <a id='toc4_4_1_4_'></a>[Conclusion](#toc0_)\n",
    "Normalization and standardization are often used in machine learning and image processing, but their effects can vary drastically. While normalization preserves the visual integrity of the image, standardization can make it unrecognizable. Therefore, the choice between normalization and standardization should be made carefully, depending on the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New normalization function\n",
    "def normalize_signal(signal):\n",
    "    return (signal - np.min(signal)) / (np.max(signal) - np.min(signal))\n",
    "\n",
    "# New standardization function\n",
    "def standardize_signal(signal):\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "# Step 1: Load the audio files\n",
    "audio_files = {\n",
    "    'Jessi_What_Type_of_X': 'audio/Jessi_What_Type_of_X_segment.wav',\n",
    "    'Everglow_Dun_Dun': 'audio/Everglow_Dun_Dun_segment.wav',\n",
    "    'Samulnori': 'audio/Samulnori_segment.wav'\n",
    "}\n",
    "\n",
    "audio_data = {}\n",
    "\n",
    "# Step 2: Normalize and Standardize\n",
    "for name, path in audio_files.items():\n",
    "    # Load the audio\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    # Store original audio and sample rate\n",
    "    audio_data[name] = {'original': y, 'sr': sr}\n",
    "\n",
    "    # Normalize audio\n",
    "    normalized_y = normalize_signal(y)\n",
    "\n",
    "    # Standardize audio\n",
    "    standardized_y = standardize_signal(normalized_y)\n",
    "\n",
    "    # Store all versions and sample rate\n",
    "    audio_data[name].update({'normalized': normalized_y, 'standardized': standardized_y})\n",
    "\n",
    "for name, data in audio_data.items():\n",
    "    sr = data['sr']\n",
    "    t = np.linspace(0, len(data['original']) / sr, len(data['original']))\n",
    "\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(18, 6))\n",
    "\n",
    "    ax[0].plot(t, data['original'])\n",
    "    ax[0].set_title(f'{name} - Original Signal')\n",
    "    ax[0].set_xlabel('Time [s]')\n",
    "    ax[0].set_ylabel('Amplitude')\n",
    "\n",
    "    ax[1].plot(t, data['normalized'], color='green')\n",
    "    ax[1].set_title(f'{name} - Normalized Signal')\n",
    "    ax[1].set_xlabel('Time [s]')\n",
    "    ax[1].set_ylabel('Amplitude')\n",
    "\n",
    "    ax[2].plot(t, data['standardized'], color='orange')\n",
    "    ax[2].set_title(f'{name} - Standardized Signal')\n",
    "    ax[2].set_xlabel('Time [s]')\n",
    "    ax[2].set_ylabel('Amplitude')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. <a id='toc4_4_2_'></a>[Analysis: Signal Normalization and Standardization](#toc0_)\n",
    "\n",
    "#### 4.4.2.1. <a id='toc4_4_2_1_'></a>[Normalization](#toc0_)\n",
    "Normalizing the signal essentially shifts its amplitude range to fit between 0 and 1. Visually, this shift doesn't change the shape or features of the waveform, only its amplitude. It's akin to a linear transformation that preserves the relative dynamics of the signal. \n",
    "\n",
    "#### 4.4.2.2. <a id='toc4_4_2_2_'></a>[Standardization](#toc0_)\n",
    "Standardization, however, does more than just shifting the center; it significantly alters the amplitudes. The waveform still maintains its shape but the amplitude extremes are pushed towards or away from the mean, based on the standard deviation. This can be critical when interpreting features like peaks or troughs, as their relative prominence can change.\n",
    "\n",
    "#### 4.4.2.3. <a id='toc4_4_2_3_'></a>[Why This Happens](#toc0_)\n",
    "Normalization is a simpler operation, just a rescaling. Standardization, however, reorganizes the data to fit a standard normal distribution with a mean of 0 and a standard deviation of 1. This transformation can significantly alter the original distribution of amplitude values, making some previously prominent features less noticeable and vice versa.\n",
    "\n",
    "#### 4.4.2.4. <a id='toc4_4_2_4_'></a>[Conclusion](#toc0_)\n",
    "Normalization and standardization serve different purposes and should be chosen based on what aspect of the data you wish to preserve or highlight. While normalization is less intrusive, standardization can drastically change the way the signal is represented and interpreted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. <a id='toc5_'></a>[convolution/filtering in image and signal (LE2)](#toc0_)\n",
    "## 5.1. <a id='toc5_1_'></a>[Filtering in the spatial domain](#toc0_)\n",
    "Implement yourself a classical algorithm for filtering signals (1D) and images (2D) in the spatial domain (convolution). What elements should such an algorithm contain? Test your function with one signal and one image each matching your country and two convolution kernels of appropriate size. The two convolution kernels should {'de-noise, sharpen'} the signal and image, respectively. Does your algorithm scale for big data? Measure the differences of your example signals/images before and after filtering using an appropriate metric. Discuss your data, method and parameter choices and the results obtained. Why did you choose this convolution kernel or metric? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. <a id='toc5_1_1_'></a>[Choice of Kernals](#toc0_)\n",
    "\n",
    "\n",
    "#### 5.1.1.1. <a id='toc5_1_1_1_'></a>[Image Filtering](#toc0_)\n",
    "1. **Laplacian Kernel**: For image and signal sharpening, a Laplacian kernel is used. The Laplacian kernel is effective at enhancing edges and fine details in the image. It does this by highlighting regions of rapid intensity change. The specific Laplacian mask chosen is:\n",
    "\n",
    "    $$\n",
    "    \\begin{bmatrix}\n",
    "    0 & -1 & 0 \\\\\n",
    "    -1 & 20 & -1 \\\\\n",
    "    0 & -1 & 0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "   \n",
    "    \n",
    "This mask has negative values surrounding the center pixel and a higher value at the center, effectively sharpening the image by emphasizing the contrast.\n",
    "\n",
    "2. **Gaussian Kernel**: For the task of de-noising, a Gaussian kernel is chosen. The Gaussian kernel effectively blurs an image, which is useful for removing high-frequency noise. It does this by averaging pixel values in a neighborhood around each pixel, weighted by the Gaussian function. \n",
    "\n",
    "    - **Kernel Size**: After experimenting, it was found that a larger kernel size can blur out more diffused noise but risks losing finer details.\n",
    "    - **Standard Deviation ($ \\sigma $)**: A smaller $\\sigma$ was effective for localized, high-frequency noise, whereas a larger $ \\sigma $ smoothed out more generalized noise without losing much detail.\n",
    "\n",
    "#### 5.1.1.2. <a id='toc5_1_1_2_'></a>[Audio Filtering](#toc0_)\n",
    "\n",
    "1. **Laplacian Kernel**: For audio sharpening, a 1D Laplacian kernel is used, represented as `[-1, 2, -1]`. This kernel serves as a discrete approximation to the second derivative and is effective in emphasizing rapid changes in the audio signal. It acts as a high-pass filter, making transitions between audio samples more distinct and thereby making the audio sound crisper. The choice of this specific mask is guided by its mathematical properties that make it effective for edge detection and signal sharpening.\n",
    "\n",
    "2. **Gaussian Kernel**: For de-noising the audio signal, a Gaussian kernel is used. This kernel smooths the audio signal by performing a weighted average on the audio samples. The weights are determined by the Gaussian function, allowing the algorithm to preserve more important details while removing noise.\n",
    "\n",
    "    - **Kernel Size**: A smaller kernel size was effective for short-lived, abrupt noise spikes in the audio, whereas a larger size smoothed out more constant noise without significantly altering the signal.\n",
    "    - **Standard Deviation ($\\sigma$)**: Experimentation showed that a smaller $\\sigma$ value effectively removed high-frequency noise spikes, while a larger $ \\sigma $ was useful for more constant and diffused noise.\n",
    "\n",
    "By experimenting with these parameters, both the Laplacian and Gaussian kernels can be finely tuned for specific de-noising and sharpening tasks in image and audio processing.\n",
    "\n",
    "### 5.1.2. <a id='toc5_1_2_'></a>[Experiment](#toc0_)\n",
    "\n",
    "#### 5.1.2.1. <a id='toc5_1_2_1_'></a>[Metric for Comparing Images and Signals](#toc0_)\n",
    "\n",
    "To quantify the effectiveness of the applied filters, the Mean Squared Error (MSE) will be used for images. This metric will help in assessing how much the original image differs from the filtered version. For signals, the Signal-to-Noise Ratio (SNR) will be employed to similarly measure the effect of the filter.\n",
    "\n",
    "#### 5.1.2.2. <a id='toc5_1_2_2_'></a>[Data for Testing](#toc0_)\n",
    "\n",
    "1. **Image Data**: We will reuse a discarded image from a previous experiment that involved air pollution removal. This image has undergone LAB color space filtering and bilateral filtering. Although it was discarded for better alternatives, it serves as a good candidate for testing:\n",
    "\n",
    "    - **Image Enhancement**: The image will first be enhanced for sharpness using the Laplacian kernel to see if the details and edges can be further improved.\n",
    "  \n",
    "    - **Image De-noising**: After sharpening, the image will be subjected to de-noising using the Gaussian kernel to remove any remaining noise artifacts.\n",
    "\n",
    "2. **Audio Data**: The audio data consists of vocals extracted from the first part of the song \"Dun Dun\". We will explore:\n",
    "\n",
    "    - **Audio Enhancement**: The aim is to see if increasing sharpness using a 1D Laplacian kernel can make the vocals sound crisper.\n",
    "    \n",
    "    - **Audio De-noising**: De-noising will be applied using a 1D Gaussian kernel to see if the vocals can be made clearer, minimizing any background noise or artifacts.\n",
    "\n",
    "By doing so, we aim to demonstrate the practical applicability and effectiveness of the chosen kernels and convolution algorithms in diverse domains.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def laplace_kernel_matrix(size: int = 1, weight: float = 1.0):\n",
    "    \"\"\"\n",
    "    Generate a scalable Laplacian kernel matrix for image sharpening.\n",
    "    \n",
    "    Parameters:\n",
    "    - size: int, determines the size of the kernel\n",
    "    - weight: float, scales the entire kernel\n",
    "    \n",
    "    Returns:\n",
    "    - kernel_matrix: 2D numpy array, Laplacian kernel matrix\n",
    "    \"\"\"\n",
    "    dim = 2 * size + 1\n",
    "    kernel_matrix = np.zeros((dim, dim))\n",
    "\n",
    "    # Set values around the center pixel to -1\n",
    "    kernel_matrix[size, size-1:size+2] = -1\n",
    "    kernel_matrix[size-1:size+2, size] = -1\n",
    "\n",
    "    # Set the diagonal values to 0\n",
    "    for i in range(size-1, size+2):\n",
    "        for j in range(size-1, size+2):\n",
    "            if i == j:\n",
    "                kernel_matrix[i, j] = 0\n",
    "\n",
    "    # Set the center pixel to 10\n",
    "    kernel_matrix[size, size] = weight\n",
    "\n",
    "    return kernel_matrix\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_kernel_1d(size: int, sigma: float):\n",
    "    \"\"\"\n",
    "    Generate a 1D Gaussian kernel for signal de-noising.\n",
    "    The kernel is of size (2*size + 1) and is controlled by the sigma parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    - size: int, determines the size of the kernel\n",
    "    - sigma: float, standard deviation for the Gaussian function\n",
    "    \n",
    "    Returns:\n",
    "    - kernel: 1D numpy array, Gaussian kernel\n",
    "    \"\"\"\n",
    "    # Create an array to store the Gaussian kernel\n",
    "    kernel = np.zeros(2*size + 1)\n",
    "\n",
    "    for x in range(-size, size+1):\n",
    "        kernel[x + size] = np.exp(-x**2 / (2 * sigma ** 2))\n",
    "\n",
    "    # Normalize the kernel\n",
    "    kernel /= np.sum(kernel)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "def gaussian_kernel_matrix(size: int, sigma: float):\n",
    "    \"\"\"\n",
    "    Generate a Gaussian kernel matrix for image de-noising.\n",
    "    The kernel is of size (2*size + 1) x (2*size + 1) and is controlled by the sigma parameter.\n",
    "    Deep Dive 1 with slight changes.\n",
    "    \n",
    "    Parameters:\n",
    "    - size: int, determines the size of the kernel\n",
    "    - sigma: float, standard deviation for the Gaussian function\n",
    "    \n",
    "    Returns:\n",
    "    - kernel_matrix: 2D numpy array, Gaussian kernel matrix\n",
    "    \"\"\"\n",
    "    dim = 2*size + 1\n",
    "    kernel_matrix = np.zeros((dim, dim))\n",
    "    for x in range(dim):\n",
    "        for y in range(dim):\n",
    "            dx = x - size\n",
    "            dy = y - size\n",
    "            kernel_matrix[x, y] = np.exp(-(dx**2 + dy**2) / (2 * sigma ** 2))\n",
    "            \n",
    "    # Normalize the kernel\n",
    "    kernel_matrix /= np.sum(kernel_matrix)\n",
    "    \n",
    "    return kernel_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convolve_1d(signal, kernel):\n",
    "    kernel_size = len(kernel)\n",
    "    pad_size = kernel_size // 2\n",
    "\n",
    "    padded_signal = np.pad(signal, pad_size, mode='constant')\n",
    "\n",
    "    # numpy black magic striding window for vectorized convolution\n",
    "    strided = np.lib.stride_tricks.sliding_window_view(padded_signal, (kernel_size,))\n",
    "\n",
    "    # Perform convolution using tensordot for optimized calculation\n",
    "    result = np.tensordot(strided, kernel, axes=([1], [0]))\n",
    "\n",
    "    return result\n",
    "\n",
    "def convolve_2d(image, kernel, padding_type='constant'):\n",
    "    kernel = np.flipud(np.fliplr(kernel))\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    pad_height = kernel_height // 2\n",
    "    pad_width = kernel_width // 2\n",
    "\n",
    "    padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width)), padding_type)\n",
    "\n",
    "    # numpy black magic striding window for vectorized convolution\n",
    "    strided = np.lib.stride_tricks.sliding_window_view(padded_image, (kernel_height, kernel_width))\n",
    "\n",
    "    # Perform the convolution operation in a vectorized manner\n",
    "    result = np.tensordot(strided, kernel, axes=([2,3], [0,1]))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laplacian_kernel = laplace_kernel_matrix(size=1, weight=20)\n",
    "gaussian_kernel = gaussian_kernel_matrix(size=1, sigma=2)\n",
    "\n",
    "laplacian_kernel, gaussian_kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_bilateral_image = processed_images_denoised['Lab Eq Image']['Bilateral Filtering']\n",
    "\n",
    "original_bilateral_image = cv2.cvtColor(original_bilateral_image, cv2.COLOR_BGR2GRAY)\n",
    "start_time = time.time()\n",
    "\n",
    "laplacian_filtered_image = original_bilateral_image.copy()\n",
    "for _ in range(7):\n",
    "    laplacian_filtered_image = convolve_2d(laplacian_filtered_image, laplacian_kernel)\n",
    "\n",
    "# Denoise the sharpened image using the Gaussian filter 4 times\n",
    "gaussian_filtered_image = laplacian_filtered_image.copy()\n",
    "for _ in range(1):\n",
    "    gaussian_filtered_image = convolve_2d(gaussian_filtered_image, gaussian_kernel)\n",
    "\n",
    "image_conv_time = time.time() - start_time\n",
    "\n",
    "image_conv_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssim_image_laplacian = ssim(original_bilateral_image, laplacian_filtered_image, data_range=laplacian_filtered_image.max() - laplacian_filtered_image.min())\n",
    "ssim_image_gaussian = ssim(original_bilateral_image, gaussian_filtered_image, data_range=gaussian_filtered_image.max() - gaussian_filtered_image.min())\n",
    "\n",
    "mse_image_laplacian = calculate_mse(original_bilateral_image, laplacian_filtered_image)\n",
    "mse_image_gaussian = calculate_mse(original_bilateral_image, gaussian_filtered_image)\n",
    "\n",
    "# plot them side by side \n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "axes[0].imshow(original_bilateral_image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(laplacian_filtered_image, cmap='gray')\n",
    "axes[1].set_title(f'Laplacian Filtered Image (Sharpened)\\nSSIM: {ssim_image_laplacian:.2f}, MSE: {mse_image_laplacian:.2f}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(gaussian_filtered_image, cmap='gray')\n",
    "axes[2].set_title(f'Laplacian and Gaussian Filtered Image (Denoised)\\nSSIM: {ssim_image_gaussian:.2f}, MSE: {mse_image_gaussian:.2f}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.3. <a id='toc5_1_2_3_'></a>[Analysis: Image Filtering in Spatial Domain](#toc0_)\n",
    "\n",
    "##### 5.1.2.3.1. <a id='toc5_1_2_3_1_'></a>[Choice of Kernels](#toc0_)\n",
    "The choice of kernels was based on the need to achieve two primary objectives: sharpening and de-noising the image. A Laplacian kernel with a central weight of 20 was used for sharpening, while a Gaussian kernel with a standard deviation of 2 was used for de-noising. Both kernels were 3x3 in size.\n",
    "\n",
    "##### 5.1.2.3.2. <a id='toc5_1_2_3_2_'></a>[Metrics: SSIM and MSE](#toc0_)\n",
    "- **SSIM for Laplacian**: 0.89\n",
    "- **MSE for Laplacian**: 157.24\n",
    "- **SSIM after Laplacian and Gaussian**: 0.96\n",
    "- **MSE after Laplacian and Gaussian**: 13.66\n",
    "\n",
    "##### 5.1.2.3.3. <a id='toc5_1_2_3_3_'></a>[Interpretation of Metrics](#toc0_)\n",
    "The SSIM value decreased slightly from 1 to 0.89 after applying the Laplacian kernel, indicating a minor loss in structural similarity due to sharpening. The MSE value was relatively high at 157.24, pointing to a considerable difference from the original image. This is consistent with the edge-enhancing effect of the Laplacian kernel.\n",
    "\n",
    "Upon applying the Gaussian kernel for de-noising, the SSIM value improved to 0.96, and the MSE reduced dramatically to 13.66. These metrics suggest that the Gaussian kernel effectively removed noise while maintaining the structural integrity of the image.\n",
    "\n",
    "##### 5.1.2.3.4. <a id='toc5_1_2_3_4_'></a>[Scalability for Big Data](#toc0_)\n",
    "The convolution function was initially implemented using nested loops. However, we later optimized the algorithm using NumPy's specialized functions like `np.tensordot` and `sliding_window_view`. This significantly improved the performance, reducing the compile time to 0.194 seconds for the test audio. These optimizations make the algorithm considerably faster than the nested loops approach and more suitable for applications requiring more efficient computation. However, true scalability is not guaranteed solely by computational efficiency; it also requires handling the increased volume of data, memory management, and parallel processing capabilities to be considered fully scalable.\n",
    "\n",
    "##### 5.1.2.3.5. <a id='toc5_1_2_3_5_'></a>[Conclusion](#toc0_)\n",
    "The convolution algorithms successfully achieved the goals of sharpening and de-noising the image. The choice of kernels and their parameters were validated both visually and quantitatively through SSIM and MSE metrics. However, further optimizations may be needed to scale the algorithm for larger datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Audio Filtering\n",
    "\n",
    "audio_clip, sr = everglow_dun_dun_vocals, everglow_dun_dun_sr\n",
    "\n",
    "gaussian_kern_1d = gaussian_kernel_1d(size=2, sigma=2.0)\n",
    "\n",
    "# Measure 1D convolution performance on the audio signal 'everglow_dun_dun_vocals'\n",
    "start_time = time.time()\n",
    "laplacian_filtered_audio = convolve_1d(audio_clip, [-1, 2, -1])\n",
    "\n",
    "gaussian_filtered_audio = audio_clip.copy()\n",
    "for _ in range(4):\n",
    "    gaussian_filtered_audio = convolve_1d(gaussian_filtered_audio, gaussian_kern_1d)\n",
    "audio_conv_time = time.time() - start_time\n",
    "audio_conv_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snr_audio_gaussian = calculate_snr(audio_clip, gaussian_filtered_audio)\n",
    "snr_audio_laplacian = calculate_snr(audio_clip, laplacian_filtered_audio)\n",
    "mse_audio_laplacian = calculate_mse(audio_clip, laplacian_filtered_audio)\n",
    "mse_audio_gaussian = calculate_mse(audio_clip, gaussian_filtered_audio)\n",
    "\n",
    "spectrogram_dict_convolution = {\n",
    "    'Original Audio': audio_clip,\n",
    "    'Laplacian Filtered Audio (Sharpened)': laplacian_filtered_audio,\n",
    "    'Gaussian Filtered Audio (Denoised)': gaussian_filtered_audio\n",
    "}\n",
    "\n",
    "plot_waveform_and_spectrogram(audio_clip, everglow_dun_dun_sr, 'Original Audio')\n",
    "plot_waveform_and_spectrogram(laplacian_filtered_audio, everglow_dun_dun_sr, f'Laplacian Filtered Audio (Sharpened) \\n SNR: {snr_audio_laplacian:.2f}, MSE: {mse_audio_laplacian:.2f}')\n",
    "plot_waveform_and_spectrogram(gaussian_filtered_audio, everglow_dun_dun_sr, f'Gaussian Filtered Audio (Denoised) \\n SNR: {snr_audio_gaussian:.2f}, MSE: {mse_audio_gaussian:.2f}')\n",
    "\n",
    "plot_metrics({'SNR': snr_audio_laplacian, 'MSE': mse_audio_laplacian}, 'Laplacian Filtered Audio (Sharpened)')\n",
    "plot_metrics({'SNR': snr_audio_gaussian, 'MSE': mse_audio_gaussian}, 'Gaussian Filtered Audio (Denoised)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.4. <a id='toc5_1_2_4_'></a>[Audio Filtering Analysis](#toc0_)\n",
    "\n",
    "##### 5.1.2.4.1. <a id='toc5_1_2_4_1_'></a>[Laplacian Filtered Audio (Sharpened)](#toc0_)\n",
    "\n",
    "1. **SNR**: The Signal-to-Noise Ratio (SNR) is negative at -7.09. A negative SNR indicates that the noise in the signal has been amplified, usually due to the sharpening effect. This suggests that the Laplacian filter, while enhancing the sharpness, might have introduced some noise or amplified existing noise in the audio signal.\n",
    "\n",
    "2. **MSE**: The Mean Squared Error (MSE) is 0.173. A higher MSE generally indicates a greater difference between the original and filtered signals. In this case, the high MSE value confirms that the Laplacian filter has substantially altered the audio, which is expected as it is designed to sharpen the audio.\n",
    "\n",
    "##### 5.1.2.4.2. <a id='toc5_1_2_4_2_'></a>[Gaussian Filtered Audio (Denoised)](#toc0_)\n",
    "\n",
    "1. **SNR**: The SNR is positive at 9.52, indicating that the signal quality has likely improved after the Gaussian filtering. A higher SNR usually suggests a cleaner, less noisy signal, which aligns with the purpose of the Gaussian filter to de-noise the audio.\n",
    "\n",
    "2. **MSE**: The MSE is significantly lower at 0.0038, suggesting that the Gaussian filter has made minimal changes to the original signal. This is expected as the Gaussian filter aims to remove noise while preserving the original signal as much as possible.\n",
    "\n",
    "##### 5.1.2.4.3. <a id='toc5_1_2_4_3_'></a>[Spectrograms](#toc0_)\n",
    "\n",
    "The spectrograms provide additional insights into the effects of the filters. The Laplacian filter shows more high-frequency components, indicative of the sharpening effect, and a clear reduction in lower frequencies. Conversely, the Gaussian filter shows a smoother spectrum, confirming its de-noising effect, and a noticeable reduction in higher frequencies.\n",
    "\n",
    "##### 5.1.2.4.4. <a id='toc5_1_2_4_4_'></a>[Scalability for Big Data](#toc0_)\n",
    "The convolution function was initially implemented using nested loops. However, we later optimized the algorithm using NumPy's specialized functions like `np.tensordot` and `sliding_window_view`. This significantly improved the performance, reducing the compile time to 0.064 seconds for the test audio. These optimizations make the algorithm considerably faster than the nested loops approach and more suitable for applications requiring more efficient computation. However, true scalability is not guaranteed solely by computational efficiency; it also requires handling the increased volume of data, memory management, and parallel processing capabilities to be considered fully scalable. \n",
    "\n",
    "##### 5.1.2.4.5. <a id='toc5_1_2_4_5_'></a>[Conclusion](#toc0_)\n",
    "\n",
    "Both the Laplacian and Gaussian filters have shown expected behavior. The Laplacian filter is effective for sharpening but at the cost of introducing or amplifying noise, as evidenced by the negative SNR. On the other hand, the Gaussian filter successfully de-noises the audio, improving the SNR and making minimal changes to the original signal, as indicated by the low MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. <a id='toc5_2_'></a>[Filtering in the spectral domain](#toc0_)\n",
    "Implement a method based on spectral filtering that filters a repetitive pattern from a signal or image. The method should work for signals (1D) as well as images (2D). Demonstrate the result using a signal and an image appropriate to your country. Discuss your data, method, and parameter choices and the results obtained. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. <a id='toc5_2_1_'></a>[Experiment 1: Redundancy Cleanup](#toc0_)\n",
    "\n",
    "The goal of this experiment is to demonstrate the effectiveness of spectral filtering in removing repetitive patterns from audio signals and images. This technique is particularly useful for signal compression and noise reduction, which are essential in various applications like data storage and transmission, image processing, and audio engineering.\n",
    "\n",
    "#### 5.2.1.1. <a id='toc5_2_1_1_'></a>[Methods](#toc0_)\n",
    "\n",
    "##### 5.2.1.1.1. <a id='toc5_2_1_1_1_'></a>[Spectral Filtering for 1D Signals](#toc0_)\n",
    "\n",
    "**Fourier Transform**: The first step is to transform the signal from the time domain to the frequency domain using the Fast Fourier Transform (FFT). This transformation allows for the identification and manipulation of specific frequency components.\n",
    "\n",
    "**Adaptive Thresholding**: An adaptive threshold based on the mean and standard deviation of the spectral magnitudes is employed to identify significant components. This method ensures that the filtering process is dynamic and can adjust to different signal characteristics.\n",
    "\n",
    "**Compression Measurement**: The compression ratio and the percentage of spectrum removal are computed as metrics to evaluate the extent of filtering and data reduction.\n",
    "\n",
    "**Inverse Fourier Transform**: Finally, the Inverse Fast Fourier Transform (IFFT) is used to revert the modified spectrum back to a time-domain signal, resulting in a filtered signal with reduced repetitive patterns.\n",
    "\n",
    "##### 5.2.1.1.2. <a id='toc5_2_1_1_2_'></a>[Spectral Filtering for 2D Images](#toc0_)\n",
    "\n",
    "**2D Fourier Transform**: Similar to 1D signals, the image is transformed to the frequency domain using the 2D FFT. This enables the analysis of spatial frequencies and the detection of repetitive patterns.\n",
    "\n",
    "**Adaptive Thresholding**: Using the same adaptive thresholding technique as in 1D signals, significant frequency components are identified and isolated for images.\n",
    "\n",
    "**Compression Measurement**: The compression ratio and the percentage of spectrum removal are calculated to assess the effectiveness of filtering with regard to pattern removal and data compression.\n",
    "\n",
    "**2D Inverse Fourier Transform**: The modified frequency-domain image is converted back to the spatial domain using the 2D IFFT, yielding an image with the repetitive patterns filtered out.\n",
    "\n",
    "#### 5.2.1.2. <a id='toc5_2_1_2_'></a>[Data](#toc0_)\n",
    "\n",
    "The image used for this task is a photograph of Gyeonghuigung, a palace located in Seoul, South Korea. Captured by Bundo Kim, the photograph is sourced from [Unsplash](https://unsplash.com/photos/brown-wooden-post-temple-with-green-roof-photo-during-daytime-Y7IilZ5VLdA).\n",
    "\n",
    "The signal used for this experiment is a 1D audio signal of the song \"Dun Dun\" by Everglow. The signal was chosen because it contains a repetitive pattern in the form of the song's chorus. This pattern can be filtered out using spectral filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_spectrum_comparison(original_spectrum, filtered_spectrum, title):\n",
    "    \"\"\"\n",
    "    Plots a comparison of the original and filtered Fourier spectra.\n",
    "\n",
    "    Args:\n",
    "        original_spectrum (numpy.ndarray): The original Fourier spectrum.\n",
    "        filtered_spectrum (numpy.ndarray): The filtered Fourier spectrum.\n",
    "        title (str): The title for the plot.\n",
    "    \"\"\"\n",
    "    # Define the color map\n",
    "    colormap = 'viridis'\n",
    "\n",
    "    # Set the scale for the color map to enhance visibility\n",
    "    original_magnitude = np.log1p(np.abs(np.fft.fftshift(original_spectrum)))\n",
    "    filtered_magnitude = np.log1p(np.abs(np.fft.fftshift(filtered_spectrum)))\n",
    "    vmin = min(np.min(original_magnitude), np.min(filtered_magnitude))\n",
    "    vmax = max(np.max(original_magnitude), np.max(filtered_magnitude))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    im1 = ax1.imshow(original_magnitude, cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "    ax1.set_title('Original Spectrum')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    im2 = ax2.imshow(filtered_magnitude, cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "    ax2.set_title('Filtered Spectrum')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    # Add color bars to the plots to interpret the colors\n",
    "    fig.colorbar(im1, ax=ax1, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    fig.colorbar(im2, ax=ax2, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adaptive_threshold_1d(spectrum, factor=1.0):\n",
    "    magnitude = np.abs(spectrum)\n",
    "    mean_mag = np.mean(magnitude)\n",
    "    std_mag = np.std(magnitude)\n",
    "    threshold = mean_mag + factor * std_mag\n",
    "    filtered_spectrum = np.where(magnitude > threshold, spectrum, 0)\n",
    "\n",
    "    return filtered_spectrum\n",
    "\n",
    "def adaptive_threshold_2d(spectrum, factor=1.0):\n",
    "    magnitude = np.abs(spectrum)\n",
    "    mean_mag = np.mean(magnitude)\n",
    "    std_mag = np.std(magnitude)\n",
    "    threshold = mean_mag + factor * std_mag\n",
    "    filtered_spectrum = np.where(magnitude > threshold, spectrum, 0)\n",
    "\n",
    "    return filtered_spectrum\n",
    "\n",
    "def calculate_compression_ratio(original_spectrum, filtered_spectrum):\n",
    "    return 1 - (np.sum(np.abs(filtered_spectrum)) / np.sum(np.abs(original_spectrum)))\n",
    "\n",
    "\n",
    "def spectral_filter_1d(signal, filter_function):\n",
    "    spectrum = np.fft.fft(signal)\n",
    "    filtered_spectrum = filter_function(spectrum)\n",
    "    filtered_signal = np.fft.ifft(filtered_spectrum)\n",
    "    compression_ratio = calculate_compression_ratio(spectrum, filtered_spectrum)\n",
    "    return np.real(filtered_signal), compression_ratio\n",
    "\n",
    "def spectral_filter_2d(image, filter_function):\n",
    "    spectrum = np.fft.fft2(image)\n",
    "    filtered_spectrum = filter_function(spectrum)\n",
    "    filtered_image = np.fft.ifft2(filtered_spectrum)\n",
    "    compression_ratio = calculate_compression_ratio(spectrum, filtered_spectrum)\n",
    "    return np.real(filtered_image), compression_ratio\n",
    "\n",
    "def remove_repetitive_pattern_1d(spectrum):\n",
    "    filtered_spectrum = adaptive_threshold_1d(spectrum, factor=1.0)\n",
    "    return filtered_spectrum\n",
    "\n",
    "def remove_repetitive_pattern_2d(spectrum):\n",
    "    filtered_spectrum = adaptive_threshold_2d(spectrum, factor=1.0)\n",
    "    return filtered_spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_path = 'images/Brown wooden post temple.jpg' \n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply Spectral Filtering\n",
    "filtered_image, compression_ratio = spectral_filter_2d(image, remove_repetitive_pattern_2d)\n",
    "\n",
    "plot_before_after(image, filtered_image, 'Original Image', f'Filtered Image \\nCompression: {compression_ratio*100:.2f}%', greyscale=True)\n",
    "plot_spectrum_comparison(fft2(image), fft2(filtered_image), 'Original and Filtered Spectrum')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio_path = 'audio/Everglow_Dun_Dun_segment.wav'\n",
    "audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Apply Spectral Filtering\n",
    "filtered_audio, compression_ratio_audio = spectral_filter_1d(audio, remove_repetitive_pattern_1d)\n",
    "\n",
    "plot_waveform_and_spectrogram(audio, sr, 'Original Audio')\n",
    "plot_waveform_and_spectrogram(filtered_audio, sr, f'Filtered Audio Compression: {compression_ratio_audio*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.3. <a id='toc5_2_1_3_'></a>[Reflection and Analysis on Spectral Filtering Task](#toc0_)\n",
    "\n",
    "##### 5.2.1.3.1. <a id='toc5_2_1_3_1_'></a>[Image Data Results](#toc0_)\n",
    "\n",
    "- **Compression**: The achieved compression ratio for the image is 82.75%, indicating a substantial reduction of the frequency spectrum.\n",
    "- **Quality**: Notably, the visual integrity of the image remains intact despite the high compression ratio. This suggests that the adaptive thresholding method effectively identified and removed redundant patterns without significantly compromising important image features.\n",
    "- **Spectral Analysis**: The before-and-after spectrum plots illustrate the success of the spectral filtering method. It is evident that a significant portion of the spectrum has been eliminated, yet the essential frequencies that constitute the core visual information of the image are conserved.\n",
    "\n",
    "##### 5.2.1.3.2. <a id='toc5_2_1_3_2_'></a>[Audio Data Results](#toc0_)\n",
    "\n",
    "- **Compression**: The audio data underwent a compression of 69%, signifying a considerable data reduction.\n",
    "- **Quality**: The main qualities of the song are retained, with the most noticeable change being a reduction in volume. This reduction could result from the filtering out of frequency components that contribute to the overall loudness.\n",
    "- **Artifacts**: The filtering process introduced a ringing artifact in the audio. This artifact, known as Gibbs ringing, arises from the abrupt cutoff in the frequency domain created by the adaptive threshold, which acts as a hard filter. When the signal is converted back to the time domain, these sharp frequency transitions can manifest as oscillatory artifacts, which are perceived as ringing or echoing. This issue is a recognized challenge in signal processing, especially when applying hard thresholds in frequency-domain filtering.\n",
    "\n",
    "##### 5.2.1.3.3. <a id='toc5_2_1_3_3_'></a>[Overall Analysis](#toc0_)\n",
    "\n",
    "The results of the experiment showcase the utility of spectral filtering in achieving significant data reduction while preserving key aspects of both images and audio. Nevertheless, the audio artifacts introduced underscore the limitations of straightforward threshold-based filtering methods. For audio, maintaining perceptual quality is crucial; thus, more nuanced approaches, such as the use of psychoacoustic models or filters with smoother transitions, might be necessary to prevent noticeable artifacts.\n",
    "\n",
    "In the context of images, the technique appears promising for scenarios where lossy compression is permissible. The method's effectiveness may vary across different images, and it may be necessary to adjust the thresholding parameters to cater to the specific characteristics of each image.\n",
    "\n",
    "The experiment highlights the need to customize the spectral filtering process to the data type and the desired outcome, carefully balancing the trade-off between compression efficiency and quality retention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. <a id='toc5_3_'></a>[Bonus: Filtering in the spatial and spectral domain.](#toc0_)\n",
    "Optionally, perform experiments with signals or images using methods that allow filtering in the spatial and spectral domains. E.g. using wavelets. Discuss your data, parameter and method choices and the results obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. <a id='toc5_3_1_'></a>[Methods and Implementation](#toc0_)\n",
    "\n",
    "#### 5.3.1.1. <a id='toc5_3_1_1_'></a>[Soft Thresholding](#toc0_)\n",
    "- **Purpose**: The `soft_threshold` function applies a soft thresholding technique to wavelet coefficients. This technique sets coefficients below a certain threshold to zero and shrinks the others by the threshold value. It's commonly used in wavelet-based denoising.\n",
    "- **Implementation**: The function takes wavelet coefficients and a threshold value, then it computes the soft thresholding by subtracting the threshold from the absolute values of the coefficients and then retains the sign.\n",
    "\n",
    "#### 5.3.1.2. <a id='toc5_3_1_2_'></a>[Wavelet Filtering for 1D (Audio)](#toc0_)\n",
    "- **Purpose**: The `wavelet_filter_1d` function applies a discrete wavelet transform to a 1D signal (audio), then soft-thresholds the detail coefficients, and finally reconstructs the signal using the inverse wavelet transform.\n",
    "- **Wavelet Choice**: The 'haar' wavelet is used for its simplicity and efficiency in computation.\n",
    "- **Thresholding**: A default threshold is calculated as half of the maximum absolute value among the detail coefficients if none is provided.\n",
    "\n",
    "#### 5.3.1.3. <a id='toc5_3_1_3_'></a>[Wavelet Filtering for 2D (Images)](#toc0_)\n",
    "- **Purpose**: Similarly, `wavelet_filter_2d` performs the 2D discrete wavelet transform on images, applies soft thresholding, and reconstructs the image.\n",
    "- **Handling Image Size**: Wavelet transforms can result in a size increase of the reconstructed image. The function, therefore, crops the output to match the original image dimensions.\n",
    "\n",
    "### 5.3.2. <a id='toc5_3_2_'></a>[Interactive Widgets](#toc0_)\n",
    "- **Interactivity**: To facilitate the exploration of the effect of thresholding on the quality of filtering, interactive sliders are provided using `ipywidgets`. This allows users to adjust the threshold in real-time and see the results.\n",
    "- **Range of Sliders**: For images, the threshold slider range is between 10 and 5000, while for audio, it's between 0.01 and 1.0, reflecting the different scales of wavelet coefficients in each domain.\n",
    "\n",
    "### 5.3.3. <a id='toc5_3_3_'></a>[Metrics for Quality Assessment](#toc0_)\n",
    "- **SSIM (Structural Similarity Index)**: Used for images to measure the similarity between the original and filtered images. It provides an indication of perceived changes in structural information, luminance, and contrast.\n",
    "- **SNR (Signal-to-Noise Ratio)**: Used for audio to assess the quality of the filtered audio signal relative to background noise.\n",
    "- **Display of Metrics**: Both metrics are displayed in the plot titles for immediate visual feedback.\n",
    "\n",
    "### 5.3.4. <a id='toc5_3_4_'></a>[Data](#toc0_)\n",
    "\n",
    "The image used for this task is a photograph of Gyeonghuigung, a palace located in Seoul, South Korea. Captured by Bundo Kim, the photograph is sourced from [Unsplash](https://unsplash.com/photos/brown-wooden-post-temple-with-green-roof-photo-during-daytime-Y7IilZ5VLdA).\n",
    "\n",
    "The signal used for this experiment is a 1D audio signal of the song \"Dun Dun\" by Everglow. The signal was chosen because it contains a repetitive pattern in the form of the song's chorus. This pattern can be filtered out using spectral filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def soft_threshold(coefficients, threshold):\n",
    "    \"\"\"\n",
    "    Apply soft thresholding to the coefficients.\n",
    "    Coefficients smaller than the threshold are set to zero.\n",
    "    Larger coefficients are reduced by the threshold amount.\n",
    "    \"\"\"\n",
    "    return np.sign(coefficients) * np.maximum(np.abs(coefficients) - threshold, 0)\n",
    "\n",
    "def wavelet_filter_1d(signal, wavelet='haar', threshold=None):\n",
    "    coeffs = pywt.wavedec(signal, wavelet)\n",
    "    if threshold is None:\n",
    "        # Set a default threshold if one isn't provided.\n",
    "        threshold = np.max(np.abs(coeffs[1])) * 0.5\n",
    "    coeffs[1:] = [soft_threshold(c, threshold) for c in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "def wavelet_filter_2d(image, wavelet='haar', threshold=None):\n",
    "    # Store original image size\n",
    "    original_size = image.shape\n",
    "\n",
    "    coeffs = pywt.wavedec2(image, wavelet)\n",
    "    if threshold is None:\n",
    "        threshold = np.max([np.max(np.abs(c)) for c in coeffs[1:]]) * 0.5\n",
    "    for i in range(1, len(coeffs)):\n",
    "        coeffs[i] = tuple(soft_threshold(c, threshold) for c in coeffs[i])\n",
    "\n",
    "    # Perform inverse wavelet transform\n",
    "    filtered_image = pywt.waverec2(coeffs, wavelet)\n",
    "\n",
    "    # Crop the image back to the original size\n",
    "    filtered_image = filtered_image[:original_size[0], :original_size[1]]\n",
    "\n",
    "    return filtered_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_image = wavelet_filter_2d(image, threshold=30)\n",
    "\n",
    "ssim_image_wavelet = ssim(image, filtered_image, data_range=filtered_image.max() - filtered_image.min())\n",
    "\n",
    "plot_before_after(image, filtered_image, 'Original Image', f'Wavelet Filtered Image \\n SSIM: {ssim_image_wavelet:.2f}', greyscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_audio = wavelet_filter_1d(audio, threshold=0.1)\n",
    "\n",
    "snr_audio_wavelet = calculate_snr(audio, filtered_audio)\n",
    "\n",
    "plot_waveform_and_spectrogram(audio, sr, 'Original Audio')\n",
    "plot_waveform_and_spectrogram(filtered_audio, sr, f'Wavelet Filtered Audio SNR: {snr_audio_wavelet:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5. <a id='toc5_3_5_'></a>[Interactive Version](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# interactive version for playing around\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def interactive_wavelet_filter_2d(threshold):\n",
    "    filtered_image = wavelet_filter_2d(image, threshold=threshold)\n",
    "    plot_before_after(image, filtered_image, 'Original Image', f'Wavelet Filtered Image {threshold:.2f} \\n SSIM: {calculate_ssim(image, filtered_image):.2f}', greyscale=True)\n",
    "\n",
    "threshold_slider_2d = FloatSlider(min=10, max=5000, step=10, value=10, description='Threshold')\n",
    "\n",
    "interactive_plot_2d = interactive(interactive_wavelet_filter_2d, threshold=threshold_slider_2d)\n",
    "display(interactive_plot_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def interactive_wavelet_filter_1d(threshold):\n",
    "    filtered_audio = wavelet_filter_1d(audio, threshold=threshold)\n",
    "    return plot_waveform_and_spectrogram(filtered_audio, sr, f'Wavelet Filtered Audio {threshold:.2f} \\n SNR: {calculate_snr(audio, filtered_audio):.2f}')\n",
    "\n",
    "    \n",
    "\n",
    "threshold_slider_1d = FloatSlider(min=0.01, max=1.0, step=0.01, value=0.1, description='Threshold')\n",
    "\n",
    "interactive_plot_1d = interactive(interactive_wavelet_filter_1d, threshold=threshold_slider_1d)\n",
    "display(interactive_plot_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5.1. <a id='toc5_3_5_1_'></a>[Reflection and Analysis on Wavelet-Based Filtering](#toc0_)\n",
    "\n",
    "##### 5.3.5.1.1. <a id='toc5_3_5_1_1_'></a>[Overview of Results](#toc0_)\n",
    "The experimentation with wavelet-based filtering yielded quantitative metrics that provided a basis for analyzing the impact of the filtering process. For images, a threshold of 30 resulted in an SSIM of 0.79, indicating a moderate level of structural similarity to the original. Visually, the image retained its global coherence, with a noticeable reduction in noise. However, this came at the cost of introducing some blur, particularly evident in areas where fine details were present. This blurring effect underscores the inherent trade-off in noise reduction techniques between noise removal and detail preservation.\n",
    "\n",
    "In the audio domain, a threshold of 0.1 resulted in an SNR of 13.84. This value suggests a significant enhancement in signal clarity compared to the noisy input. However, a persistent noise floor was noticeable throughout the audio sample. The spectrogram visualization made it evident that while the noise was reduced, it was not completely eliminated, especially in the lower frequency bands. This reveals the limitations of a uniform threshold in dealing with noise that is not evenly distributed across the frequency spectrum.\n",
    "\n",
    "##### 5.3.5.1.2. <a id='toc5_3_5_1_2_'></a>[Haar Wavelet Justification](#toc0_)\n",
    "The 'haar' wavelet was employed in this experiment due to its simplicity and computational efficiency, making it a suitable choice for proof-of-concept implementations. Its binary split nature offers a straightforward approach to signal decomposition, which can be particularly advantageous when dealing with simple signal structures or when a quick analysis is required. Moreover, the 'haar' wavelet’s ability to capture sudden changes in signals made it a reasonable choice for this initial investigation into wavelet-based filtering.\n",
    "\n",
    "##### 5.3.5.1.3. <a id='toc5_3_5_1_3_'></a>[Interpretation of Metrics](#toc0_)\n",
    "The SSIM value of 0.79 for the image suggests that while the filtering process did not perfectly preserve the original structure, it maintained a high degree of similarity. The slight blurriness observed can be attributed to the loss of high-frequency components, which are often mistaken for noise but are, in reality, responsible for sharp transitions and edges.\n",
    "\n",
    "For the audio signal, the SNR of 13.84 reflects an improvement but also indicates that the noise is not uniformly distributed, hence the persistent noise across the segment. The presence of noise throughout the audio segment, as indicated by the spectrogram, points to the need for a more nuanced approach to thresholding, possibly involving adaptive or signal-dependent threshold levels.\n",
    "\n",
    "##### 5.3.5.1.4. <a id='toc5_3_5_1_4_'></a>[Conclusions and Future Directions](#toc0_)\n",
    "The experiment with wavelet-based filtering has provided valuable insights into the effectiveness and limitations of this approach. While the 'haar' wavelet and a simple thresholding method can achieve reasonable noise reduction, there is a clear scope for optimization. Future work could involve exploring more sophisticated wavelets that may offer better noise reduction while preserving more details. Additionally, adaptive thresholding methods that account for the signal's characteristics could be investigated to further enhance the quality of the filtered output.\n",
    "\n",
    "The results also indicate that while SSIM and SNR are useful metrics, they might not fully capture the perceptual quality experienced by users. \n",
    "\n",
    "In conclusion, the wavelet-based filtering experiment has demonstrated the potential of wavelets in denoising applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. <a id='toc5_4_'></a>[Algorithms for the detection of structures in images](#toc0_)\n",
    "Show the individual steps of a known classical image processing algorithm for the detection of {'lines'} in a suitable example image of your country. The individual steps may be self-programmed or used by libraries. It is important that intermediate results of the steps are evident. Also, describe what is conceptually relevant in each step. Discuss your choice of data, methods, parameters, and the results obtained. For which cases does your algorithm work well or poorly? Why did you choose this image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.4.1. <a id='toc5_4_1_'></a>[Line Detection Objective](#toc0_)\n",
    "\n",
    "The objective in line detection within complex images, such as the architectural ensemble of Gyeonghuigung Palace, is to discern the foundational layout from an intricate array of lines. This process aims to:\n",
    "\n",
    "- **Outline Essential Structures**: Capture the primary shapes and lines that constitute the architectural backbone.\n",
    "- **Clarify Composition**: Streamline the image by filtering out non-essential details, thus clarifying the overall composition.\n",
    "- **Enable Structural Analysis**: Offer a simplified version of the image for further analysis or applications where the primary layout is of interest.\n",
    "\n",
    "\n",
    "\n",
    "#### 5.4.1.1. <a id='toc5_4_1_1_'></a>[Methods](#toc0_)\n",
    "1. **Grayscale Conversion**: Simplifies the image by eliminating color information, focusing solely on intensity.\n",
    "2. **Gaussian Blur**: Reduces noise and detail in the image, making edge detection more effective.\n",
    "3. **Canny Edge Detection**: Identifies the edges in the image.\n",
    "4. **Probabilistic Hough Line Transform**: Detects lines based on the identified edges.\n",
    "\n",
    "#### 5.4.1.2. <a id='toc5_4_1_2_'></a>[Parameters](#toc0_)\n",
    "1. **Gaussian Blur**: Kernel size of (17, 17) was chosen to sufficiently blur the image without losing important features.\n",
    "2. **Canny Edge Detection**: Threshold values 50 and 100 define the gradient intensities for edge tracing.\n",
    "3. **Probabilistic Hough Line Transform**: \n",
    "    - `Threshold`: 90, the minimum number of intersections to detect a line.\n",
    "    - `minLineLength`: 50, the minimum length of the line.\n",
    "    - `maxLineGap`: 50, the maximum gap between segmented lines to consider them as a single line.\n",
    "\n",
    "#### 5.4.1.3. <a id='toc5_4_1_3_'></a>[Data](#toc0_)\n",
    "\n",
    "The image used for this task is a photograph of Gyeonghuigung, a palace located in Seoul, South Korea. Captured by Bundo Kim, the photograph is sourced from [Unsplash](https://unsplash.com/photos/brown-wooden-post-temple-with-green-roof-photo-during-daytime-Y7IilZ5VLdA). This image is particularly well-suited for line detection algorithms because it is rich in architectural elements. It features wooden posts, intricate roof designs, railings, and other architectural details that form multiple lines at various angles and lengths. Furthermore, the image's depth, created by the layered arrangement of these elements, adds an additional layer of complexity to the line detection task. These characteristics make this image an excellent choice for evaluating the line detection algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_intermediate_steps(image_dict, main_title=None):\n",
    "    \"\"\"\n",
    "    Visualize and describe the intermediate steps in a subplot.\n",
    "    The image_dict should have the format: {'Title': image_data}\n",
    "    \"\"\"\n",
    "    num_images = len(image_dict)\n",
    "    plt.figure(figsize=(24, 10))\n",
    "\n",
    "    if main_title:\n",
    "        plt.suptitle(main_title, fontsize=16)\n",
    "\n",
    "    for i, (title, image_data) in enumerate(image_dict.items()):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    plt.show()\n",
    "\n",
    "img_path = 'images/Brown wooden post temple.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "plot_image_and_histogram_separated(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.title('Grayscale Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blurred = cv2.GaussianBlur(gray, (17, 17),  0)\n",
    "plt.imshow(blurred, cmap='gray')\n",
    "plt.title('Blurred Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edges = cv2.Canny(blurred, 50,  100)\n",
    "\n",
    "plt.imshow(edges,   cmap='gray')\n",
    "plt.title('Edges Detected')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 90\n",
    "minLineLength = 50\n",
    "maxLineGap = 50\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=threshold, minLineLength=minLineLength, maxLineGap=maxLineGap)\n",
    "lines_on_black = np.zeros_like(img)\n",
    "\n",
    "line_detection_result = img.copy()\n",
    "\n",
    "for line in lines:\n",
    "    x1, y1, x2, y2 = line[0]\n",
    "    cv2.line(lines_on_black, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.line(line_detection_result, (x1, y1), (x2, y2), (255, 255, 0), 3)\n",
    "    \n",
    "plt.imshow(cv2.cvtColor(lines_on_black, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Lines on Black Background\\nThreshold: {threshold}\\nMinLength: {minLineLength}\\nMaxGap: {maxLineGap}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_dict = {\n",
    "    'Original Image': img,\n",
    "    'Blurred': blurred,\n",
    "    'Edges': edges,\n",
    "    'Lines on Black Background': lines_on_black,\n",
    "    'Lines on Original Image': line_detection_result\n",
    "}\n",
    "\n",
    "visualize_intermediate_steps(image_dict, main_title='Image Processing Steps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1.4. <a id='toc5_4_1_4_'></a>[Reflection and Analysis on Line Detection Results](#toc0_)\n",
    "\n",
    "##### 5.4.1.4.1. <a id='toc5_4_1_4_1_'></a>[Results Overview](#toc0_)\n",
    "The line detection algorithm applied to the Gyeonghuigung Palace image produced distinct lines overlaying the original image. The architectural elements, especially the clear structures, were highlighted effectively, demonstrating the algorithm's proficiency in identifying lines within complex images.\n",
    "\n",
    "##### 5.4.1.4.2. <a id='toc5_4_1_4_2_'></a>[Methods Effectiveness](#toc0_)\n",
    "- **Grayscale Conversion** served its purpose well by simplifying the image analysis to intensity variations.\n",
    "- **Gaussian Blur**, with its significant kernel size, efficiently reduced noise but may have slightly blurred out finer details.\n",
    "- **Canny Edge Detection** capably detected the prominent edges, preparing the image for accurate line detection.\n",
    "- **Probabilistic Hough Line Transform** adeptly identified the major lines. The chosen parameters emphasized the detection of clear structures, which is evident in the well-defined lines captured in the final output.\n",
    "\n",
    "##### 5.4.1.4.3. <a id='toc5_4_1_4_3_'></a>[Algorithm Performance](#toc0_)\n",
    "- The algorithm excelled in detecting clear structures, with the lines closely corresponding to the most prominent features of the palace.\n",
    "- Smaller details in the background, which also appear as lines, were less consistently detected, suggesting a potential area for refinement in the algorithm's sensitivity to such features.\n",
    "\n",
    "##### 5.4.1.4.4. <a id='toc5_4_1_4_4_'></a>[Limitations and Areas for Improvement](#toc0_)\n",
    "- The emphasis on longer, prominent lines meant that some smaller, yet potentially significant, details were overlooked.\n",
    "- The background details that present as lines could be better addressed by fine-tuning the parameters, possibly by adjusting the thresholds used in edge detection or by varying the line detection sensitivity in the Hough Transform.\n",
    "- Further blurring might be considered to eliminate the detection of these\n",
    "- The parameters used here were set with try and error, this could also be improved with a more sophisticated approach to threshold handling and parameter search. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
